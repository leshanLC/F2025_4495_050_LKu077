{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cc7123-ccc6-4401-964b-9f456926ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\lesha\\anaconda3\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\lesha\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\lesha\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d63ebf42-5151-4807-87fc-1a3716142201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Indeed Canada Job Scraper...\n",
      "Navigating to: https://ca.indeed.com/jobs?q=&l=Canada\n",
      "Found 15 job cards on this page\n",
      "Page 1/2 completed. Found 15 jobs so far.\n",
      "Navigating to: https://ca.indeed.com/jobs?q=&l=Canada&start=10\n",
      "Job cards not found, trying to continue...\n",
      "Found 0 job cards on this page\n",
      "Page 2/2 completed. Found 15 jobs so far.\n",
      "Successfully saved 15 jobs to canada_developer_jobs.csv\n",
      "\n",
      "First 5 job listings:\n",
      "\n",
      "1. Online ESL Tutors for Young Kids\n",
      "   Company: N/A\n",
      "   Location: N/A\n",
      "   Salary: Not specified\n",
      "\n",
      "2. Destination Specialist\n",
      "   Company: N/A\n",
      "   Location: N/A\n",
      "   Salary: Not specified\n",
      "\n",
      "3. Concierge (Entretien mÃ©nager)\n",
      "   Company: N/A\n",
      "   Location: N/A\n",
      "   Salary: Not specified\n",
      "\n",
      "4. Armed Guard\n",
      "   Company: N/A\n",
      "   Location: N/A\n",
      "   Salary: Not specified\n",
      "\n",
      "5. Journalier\n",
      "   Company: N/A\n",
      "   Location: N/A\n",
      "   Salary: Not specified\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "class IndeedSeleniumScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        self.options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        self.options.add_experimental_option('useAutomationExtension', False)\n",
    "        self.options.add_argument('--disable-extensions')\n",
    "        self.options.add_argument('--disable-gpu')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # Real user agent\n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        # Hide automation detection\n",
    "        self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "        self.jobs_data = []\n",
    "    \n",
    "    def scrape_jobs(self, job_title=\"software developer\", location=\"Toronto, ON\", max_pages=3):\n",
    "        \"\"\"Scrape jobs using Selenium\"\"\"\n",
    "        \n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                start_param = page * 10\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={start_param}\"\n",
    "                \n",
    "                print(f\"Navigating to: {url}\")\n",
    "                self.driver.get(url)\n",
    "                \n",
    "                # Wait for page to load\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Check for CAPTCHA\n",
    "                if self._check_captcha():\n",
    "                    print(\"CAPTCHA detected! Please solve it manually or try again later.\")\n",
    "                    break\n",
    "                \n",
    "                # Wait for job cards to load\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon, .cardOutline, .jobsearch-SerpJobCard'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(\"Job cards not found, trying to continue...\")\n",
    "                \n",
    "                # Extract jobs from current page\n",
    "                self._extract_page_jobs()\n",
    "                \n",
    "                print(f\"Page {page + 1}/{max_pages} completed. Found {len(self.jobs_data)} jobs so far.\")\n",
    "                \n",
    "                # Random delay between pages\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "        \n",
    "        return self.jobs_data\n",
    "    \n",
    "    def _check_captcha(self):\n",
    "        \"\"\"Check if CAPTCHA is present\"\"\"\n",
    "        try:\n",
    "            captcha_elements = self.driver.find_elements(By.ID, 'captcha')\n",
    "            captcha_input = self.driver.find_elements(By.ID, 'captcha-input')\n",
    "            return len(captcha_elements) > 0 or len(captcha_input) > 0\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _extract_page_jobs(self):\n",
    "        \"\"\"Extract jobs from the current page\"\"\"\n",
    "        try:\n",
    "            # Try multiple selectors for job cards\n",
    "            selectors = [\n",
    "                'div.job_seen_beacon',\n",
    "                'div.cardOutline',\n",
    "                'div.jobsearch-SerpJobCard',\n",
    "                'div[data-jk]'\n",
    "            ]\n",
    "            \n",
    "            job_cards = []\n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    job_cards = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if job_cards:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Found {len(job_cards)} job cards on this page\")\n",
    "            \n",
    "            for card in job_cards:\n",
    "                job_info = self._extract_job_info(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs from page: {e}\")\n",
    "    \n",
    "    def _extract_job_info(self, card):\n",
    "        \"\"\"Extract information from a single job card\"\"\"\n",
    "        try:\n",
    "            # Title\n",
    "            try:\n",
    "                title_elem = card.find_element(By.CSS_SELECTOR, 'h2.jobTitle, h2.title, a.jcs-JobTitle')\n",
    "                title = title_elem.text.strip()\n",
    "            except:\n",
    "                title = \"N/A\"\n",
    "            \n",
    "            # Company\n",
    "            try:\n",
    "                company_elem = card.find_element(By.CSS_SELECTOR, 'span.companyName, span.company')\n",
    "                company = company_elem.text.strip()\n",
    "            except:\n",
    "                company = \"N/A\"\n",
    "            \n",
    "            # Location\n",
    "            try:\n",
    "                location_elem = card.find_element(By.CSS_SELECTOR, 'div.companyLocation, div.location')\n",
    "                location = location_elem.text.strip()\n",
    "            except:\n",
    "                location = \"N/A\"\n",
    "            \n",
    "            # Salary\n",
    "            try:\n",
    "                salary_elem = card.find_element(By.CSS_SELECTOR, 'div.salary-snippet, span.salaryText')\n",
    "                salary = salary_elem.text.strip()\n",
    "            except:\n",
    "                salary = \"Not specified\"\n",
    "            \n",
    "            # Link\n",
    "            try:\n",
    "                link_elem = card.find_element(By.CSS_SELECTOR, 'a.jcs-JobTitle, a.jobtitle')\n",
    "                link = link_elem.get_attribute('href')\n",
    "                if link and link.startswith('/'):\n",
    "                    link = 'https://ca.indeed.com' + link\n",
    "            except:\n",
    "                link = \"N/A\"\n",
    "            \n",
    "            # Date\n",
    "            try:\n",
    "                date_elem = card.find_element(By.CSS_SELECTOR, 'span.date, span.datePosted')\n",
    "                date_posted = date_elem.text.strip()\n",
    "            except:\n",
    "                date_posted = \"N/A\"\n",
    "            \n",
    "            job_info = {\n",
    "                'title': title,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'salary': salary,\n",
    "                'link': link,\n",
    "                'date_posted': date_posted,\n",
    "                'scraped_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "            return job_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_to_csv(self, filename=\"indeed_jobs_selenium.csv\"):\n",
    "        \"\"\"Save results to CSV\"\"\"\n",
    "        if not self.jobs_data:\n",
    "            print(\"No jobs data to save.\")\n",
    "            return False\n",
    "        \n",
    "        df = pd.DataFrame(self.jobs_data)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Successfully saved {len(self.jobs_data)} jobs to {filename}\")\n",
    "        return True\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Indeed Canada Job Scraper...\")\n",
    "    \n",
    "    # Initialize scraper (set headless=False to see the browser)\n",
    "    scraper = IndeedSeleniumScraper(headless=False)  # Set to True to run in background\n",
    "    \n",
    "    # Scrape jobs\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=\"\",\n",
    "        location=\"Canada\",\n",
    "        max_pages=2  # Start with 2 pages\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    if jobs:\n",
    "        success = scraper.save_to_csv(\"canada_developer_jobs.csv\")\n",
    "        if success:\n",
    "            print(\"\\nFirst 5 job listings:\")\n",
    "            for i, job in enumerate(jobs[:5], 1):\n",
    "                print(f\"\\n{i}. {job['title']}\")\n",
    "                print(f\"   Company: {job['company']}\")\n",
    "                print(f\"   Location: {job['location']}\")\n",
    "                print(f\"   Salary: {job['salary']}\")\n",
    "    else:\n",
    "        print(\"No jobs were scraped. Please check:\")\n",
    "        print(\"1. Your internet connection\")\n",
    "        print(\"2. If Indeed is blocking requests\")\n",
    "        print(\"3. Try running with headless=False to see what's happening\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f8cd63-41d8-4f18-ae7f-e58e68b26cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Final Indeed Canada Job Scraper...\n",
      "=== TRYING CSS SELECTOR APPROACH ===\n",
      "Navigating to: https://ca.indeed.com/jobs?q=developer&l=Canada\n",
      "Found 16 job cards\n",
      "1. Full Stack .Net Developer... | KAnand Corporation | Remote\n",
      "2. Business Developer (Mandarin/Cantonese/V... | UNI-ONE FOOD GROUP INC. | Calgary, AB\n",
      "3. Wireless DSP Software Developer... | Skycope C-UAS Technologies Inc | Burnaby, BC V5G 1J9\n",
      "4. Software Engineer - Applications... | CanCap Management Inc. | Hybrid work in Toronto, ON M5E 1M2\n",
      "5. Robot Programmer... | TransCanada Turbines | Rocky View County, AB\n",
      "6. Sr Embedded Developer - Security Special... | Dormakaba Group | MontrÃ©al, QC\n",
      "7. Business Developer - Canada... | Roullier | Ontario\n",
      "8. Senior Geophysical Java Software Enginee... | Halliburton | Calgary, AB T2P 3V4\n",
      "9. Shopify Developer... | Summit Tools | Burnaby, BC\n",
      "10. Frontend AI Engineer (Remote - EST Time ... | Funded.club | Toronto, ON\n",
      "11. ... |  | \n",
      "12. Associate, Software Engineer, New Grad... | Capital One - CA | Hybrid work in Toronto, ON M5R 3V5\n",
      "13. Student -Web Developer... | ABM College | Calgary, AB T2A 6J9\n",
      "14. Software Engineer - API... | CMiC | Hybrid work in Toronto, ON M3J 3K1\n",
      "15. GenAI Developer with Palantir... | Capgemini | Toronto, ON\n",
      "16. Full Stack Developer... | Webistry | Hybrid work in MontrÃ©al, QC H4N 1H2\n",
      "CSS approach saved 16 jobs\n",
      "\n",
      "CSS Selector Results:\n",
      "1. Full Stack .Net Developer\n",
      "   Company: KAnand Corporation\n",
      "   Location: Remote\n",
      "   Salary: $80â€“$100 an hour\n",
      "\n",
      "2. Business Developer (Mandarin/Cantonese/Vietnamese/Korean Speaking)\n",
      "   Company: UNI-ONE FOOD GROUP INC.\n",
      "   Location: Calgary, AB\n",
      "   Salary: From $42,000 a year\n",
      "\n",
      "3. Wireless DSP Software Developer\n",
      "   Company: Skycope C-UAS Technologies Inc\n",
      "   Location: Burnaby, BC V5G 1J9\n",
      "   Salary: $100,000â€“$150,000 a year\n",
      "\n",
      "\n",
      "=== TRYING PRECISE TEXT ANALYSIS APPROACH ===\n",
      "\n",
      "=== PAGE 1 ===\n",
      "Navigating to: https://ca.indeed.com/jobs?q=developer&l=Canada\n",
      "Found 16 job cards\n",
      "  âœ“ Full Stack .Net Developer... | Easily apply | Full Stack .Net Developer\n",
      "  âœ“ Business Developer (Mandarin/Cantonese/V... | Paid time off | Business Developer (Mandarin/Cantonese/Vietnamese/Korean Speaking)\n",
      "  âœ“ Wireless DSP Software Developer... | Paid time off | Wireless DSP Software Developer\n",
      "  âœ“ Software Engineer - Applications... | Hybrid work in Toronto, ON M5E 1M2 | Software Engineer - Applications\n",
      "  âœ— Failed to extract job 5\n",
      "  âœ“ Sr Embedded Developer - Security Special... | MontrÃ©al, QC | Sr Embedded Developer - Security Specialist/DÃ©veloppeur embarquÃ© senior - SpÃ©cialiste de la sÃ©curitÃ©\n",
      "  âœ“ Senior Geophysical Java Software Enginee... | Calgary, AB T2P 3V4 | Halliburton\n",
      "  âœ“ Shopify Developer... | Summit Tools | Shopify Developer\n",
      "  âœ“ Frontend AI Engineer (Remote - EST Time ... | Funded.club | Frontend AI Engineer (Remote - EST Time Zone)\n",
      "  âœ“ Full Stack Developer... | Hybrid work in Burnaby, BC V5G 4T1 | Full Stack Developer\n",
      "  âœ“ Student -Web Developer... | Flexible schedule | Student -Web Developer\n",
      "  âœ“ Jr. Software Developer (Vancouver Island... | Plexxis Software | Jr. Software Developer (Vancouver Island residents ONLY)\n",
      "  âœ“ Software Engineer - API... | CMiC | Hybrid work in Toronto, ON M3J 3K1\n",
      "  âœ“ Tekla Extensions Developer... | Stubbe s Workforce Inc | Tekla Extensions Developer\n",
      "  âœ“ CMM Programmer/ Operator... | Guelph, ON N1H 1G2 | CMM Programmer/ Operator\n",
      "  âœ“ Full Stack Developer... | Webistry | Full Stack Developer\n",
      "Added 15 jobs from page 1. Total: 15\n",
      "Successfully saved 15 jobs to precise_analysis_jobs.csv\n",
      "\n",
      "Precise analysis saved 15 jobs\n",
      "\n",
      "Precise Analysis Results:\n",
      "1. Full Stack .Net Developer\n",
      "   Company: Easily apply\n",
      "   Location: Full Stack .Net Developer\n",
      "   Salary: $80â€“$100 an hour\n",
      "\n",
      "2. Business Developer (Mandarin/Cantonese/Vietnamese/Korean Speaking)\n",
      "   Company: Paid time off\n",
      "   Location: Business Developer (Mandarin/Cantonese/Vietnamese/Korean Speaking)\n",
      "   Salary: From $42,000 a year\n",
      "\n",
      "3. Wireless DSP Software Developer\n",
      "   Company: Paid time off\n",
      "   Location: Wireless DSP Software Developer\n",
      "   Salary: $100,000â€“$150,000 a year\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "class IndeedCanadaScraper:\n",
    "    def __init__(self, headless=False):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "    \n",
    "    def scrape_jobs(self, job_title=\"developer\", location=\"Canada\", max_pages=2):\n",
    "        \"\"\"Scrape jobs with precise extraction based on observed structure\"\"\"\n",
    "        \n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"\\n=== PAGE {page + 1} ===\")\n",
    "                print(f\"Navigating to: {url}\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(4)\n",
    "                \n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(\"Job cards not found, continuing anyway...\")\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_precise()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"Added {jobs_added} jobs from page {page + 1}. Total: {len(self.jobs_data)}\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _extract_page_jobs_precise(self):\n",
    "        \"\"\"Extract jobs with precise logic based on observed structure\"\"\"\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for i, card in enumerate(job_cards):\n",
    "                job_info = self._extract_job_info_precise(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    print(f\"  âœ“ {job_info['title'][:40]}... | {job_info['company']} | {job_info['location']}\")\n",
    "                else:\n",
    "                    print(f\"  âœ— Failed to extract job {i+1}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs from page: {e}\")\n",
    "\n",
    "    def _extract_job_info_precise(self, card):\n",
    "        \"\"\"Precise job extraction based on the observed structure\"\"\"\n",
    "        try:\n",
    "            # Get all text content\n",
    "            card_text = card.text\n",
    "            lines = [line.strip() for line in card_text.split('\\n') if line.strip()]\n",
    "            \n",
    "            if not lines:\n",
    "                return None\n",
    "            \n",
    "            # Extract components based on the observed pattern:\n",
    "            # Pattern from analysis:\n",
    "            # 0: 'Full Stack .Net Developer'      <- Title\n",
    "            # 1: 'New'                            <- Status (ignore)\n",
    "            # 2: 'Urgently hiring'                <- Hiring status (ignore)\n",
    "            # 3: 'KAnand Corporation'             <- Company\n",
    "            # 4: 'Remote'                         <- Location\n",
    "            # 5: '$80â€“$100 an hour'               <- Salary\n",
    "            # 6+: Other info\n",
    "            \n",
    "            title = self._extract_title_precise(card, lines)\n",
    "            company = self._extract_company_precise(lines)\n",
    "            location = self._extract_location_precise(lines)\n",
    "            salary = self._extract_salary_precise(lines)\n",
    "            link = self._extract_link_precise(card)\n",
    "            \n",
    "            if title and title != \"N/A\":\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company,\n",
    "                    'location': location,\n",
    "                    'salary': salary,\n",
    "                    'link': link,\n",
    "                    'scraped_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _extract_title_precise(self, card, lines):\n",
    "        \"\"\"Extract title - always first line\"\"\"\n",
    "        try:\n",
    "            # Title is always the first line after cleaning\n",
    "            title_elem = card.find_element(By.CSS_SELECTOR, 'h2.jobTitle')\n",
    "            title = title_elem.text.strip()\n",
    "            # Remove \"New\" prefix if present\n",
    "            title = re.sub(r'^New\\s*', '', title)\n",
    "            return title\n",
    "        except:\n",
    "            # Fallback: use first line\n",
    "            return lines[0] if lines else \"N/A\"\n",
    "\n",
    "    def _extract_company_precise(self, lines):\n",
    "        \"\"\"Extract company based on observed pattern\"\"\"\n",
    "        # Based on analysis, company appears at different positions\n",
    "        # Common patterns observed:\n",
    "        # - After \"Urgently hiring\" or \"New\"\n",
    "        # - Sometimes has rating numbers (3.2, 3.8, etc.)\n",
    "        # - Sometimes is the actual company name\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line_lower = line.lower()\n",
    "            \n",
    "            # Skip obvious non-company lines\n",
    "            if any(word in line_lower for word in ['new', 'urgently hiring', 'just posted', 'today', 'active']):\n",
    "                continue\n",
    "                \n",
    "            # If we find \"Urgently hiring\", company is usually the next line\n",
    "            if 'urgently hiring' in line_lower and i + 1 < len(lines):\n",
    "                return lines[i + 1]\n",
    "            \n",
    "            # If we find a rating pattern (like \"3.2\"), company might be nearby\n",
    "            if re.match(r'^\\d+\\.\\d+$', line):\n",
    "                if i + 1 < len(lines):\n",
    "                    return lines[i + 1]  # Company after rating\n",
    "                elif i - 1 >= 0:\n",
    "                    return lines[i - 1]  # Company before rating\n",
    "            \n",
    "            # Company names are usually not the title and not locations\n",
    "            if (i > 0 and  # Not the first line (title)\n",
    "                len(line) > 2 and len(line) < 60 and  # Reasonable length\n",
    "                not any(province in line.upper() for province in ['AB', 'BC', 'MB', 'NB', 'NL', 'NS', 'NT', 'NU', 'ON', 'PE', 'QC', 'SK', 'YT']) and  # Not location\n",
    "                not re.search(r'\\$[\\d,]+', line) and  # Not salary\n",
    "                not any(word in line_lower for word in ['remote', 'hybrid', 'full-time', 'part-time', 'temporary', 'contract']) and  # Not job type\n",
    "                ',' not in line):  # Not location with comma\n",
    "                return line\n",
    "        \n",
    "        return \"Company not found\"\n",
    "\n",
    "    def _extract_location_precise(self, lines):\n",
    "        \"\"\"Extract location based on observed pattern\"\"\"\n",
    "        canadian_provinces = ['AB', 'BC', 'MB', 'NB', 'NL', 'NS', 'NT', 'NU', 'ON', 'PE', 'QC', 'SK', 'YT']\n",
    "        \n",
    "        for line in lines:\n",
    "            line_upper = line.upper()\n",
    "            line_lower = line.lower()\n",
    "            \n",
    "            # Direct location indicators\n",
    "            if any(word in line_lower for word in ['remote', 'hybrid', 'on-site']):\n",
    "                return line\n",
    "                \n",
    "            # Canadian province codes\n",
    "            if any(province in line_upper for province in canadian_provinces):\n",
    "                return line\n",
    "                \n",
    "            # City, Province pattern\n",
    "            if re.search(r'.*,\\s*(AB|BC|MB|NB|NL|NS|NT|NU|ON|PE|QC|SK|YT)', line, re.IGNORECASE):\n",
    "                return line\n",
    "        \n",
    "        # Try to find location by elimination\n",
    "        for line in lines:\n",
    "            if (len(line) < 30 and \n",
    "                not any(word in line.lower() for word in ['new', 'urgently', 'hiring', 'apply', 'easily']) and\n",
    "                not re.search(r'\\$[\\d,]+', line) and\n",
    "                not re.match(r'^\\d+\\.\\d+$', line)):\n",
    "                # This might be a location\n",
    "                return line\n",
    "                \n",
    "        return \"Location not specified\"\n",
    "\n",
    "    def _extract_salary_precise(self, lines):\n",
    "        \"\"\"Extract salary information\"\"\"\n",
    "        salary_patterns = [\n",
    "            r'\\$\\d{1,3}(?:,\\d{3})*\\s*(?:a year|an hour|a month|a week)',\n",
    "            r'\\$\\d{1,3}(?:,\\d{3})*\\s*-\\s*\\$\\d{1,3}(?:,\\d{3})*\\s*(?:a year|an hour|a month|a week)',\n",
    "            r'\\$\\d+(?:\\.\\d+)?\\s*-\\s*\\$\\d+(?:\\.\\d+)?\\s*(?:a year|an hour|a month|a week)',\n",
    "            r'From \\$\\d{1,3}(?:,\\d{3})*\\s*(?:a year|an hour|a month|a week)'\n",
    "        ]\n",
    "        \n",
    "        for line in lines:\n",
    "            for pattern in salary_patterns:\n",
    "                if re.search(pattern, line, re.IGNORECASE):\n",
    "                    return line\n",
    "        \n",
    "        return \"Salary not specified\"\n",
    "\n",
    "    def _extract_link_precise(self, card):\n",
    "        \"\"\"Extract job link\"\"\"\n",
    "        try:\n",
    "            link_elem = card.find_element(By.CSS_SELECTOR, 'h2.jobTitle a, a.jcs-JobTitle')\n",
    "            link = link_elem.get_attribute('href')\n",
    "            if link:\n",
    "                if link.startswith('/'):\n",
    "                    return 'https://ca.indeed.com' + link\n",
    "                return link\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            job_id_elem = card.find_element(By.CSS_SELECTOR, 'a[data-jk]')\n",
    "            job_id = job_id_elem.get_attribute('data-jk')\n",
    "            if job_id:\n",
    "                return f'https://ca.indeed.com/viewjob?jk={job_id}'\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return \"Link not available\"\n",
    "\n",
    "    def save_to_csv(self, filename=\"indeed_canada_jobs_final.csv\"):\n",
    "        \"\"\"Save results to CSV\"\"\"\n",
    "        if not self.jobs_data:\n",
    "            print(\"No jobs data to save.\")\n",
    "            return False\n",
    "        \n",
    "        df = pd.DataFrame(self.jobs_data)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Successfully saved {len(self.jobs_data)} jobs to {filename}\")\n",
    "        return True\n",
    "\n",
    "# Alternative: Use CSS selectors for more reliable extraction\n",
    "def scrape_with_css_selectors():\n",
    "    \"\"\"Try using specific CSS selectors for more reliable extraction\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    \n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "    \n",
    "    jobs_data = []\n",
    "    \n",
    "    try:\n",
    "        url = \"https://ca.indeed.com/jobs?q=developer&l=Canada\"\n",
    "        print(f\"Navigating to: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(4)\n",
    "        \n",
    "        job_cards = driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "        print(f\"Found {len(job_cards)} job cards\")\n",
    "        \n",
    "        for i, card in enumerate(job_cards):\n",
    "            try:\n",
    "                # Title - reliable\n",
    "                title_elem = card.find_element(By.CSS_SELECTOR, 'h2.jobTitle')\n",
    "                title = title_elem.text.strip()\n",
    "                title = re.sub(r'^New\\s*', '', title)\n",
    "                \n",
    "                # Company - try multiple selectors\n",
    "                company = \"Company not found\"\n",
    "                company_selectors = [\n",
    "                    '[data-testid=\"company-name\"]',\n",
    "                    '[class*=\"companyName\"]',\n",
    "                    '[class*=\"company\"]'\n",
    "                ]\n",
    "                for selector in company_selectors:\n",
    "                    try:\n",
    "                        company_elem = card.find_element(By.CSS_SELECTOR, selector)\n",
    "                        company = company_elem.text.strip()\n",
    "                        if company:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Location - try multiple selectors\n",
    "                location = \"Location not specified\"\n",
    "                location_selectors = [\n",
    "                    '[data-testid=\"text-location\"]',\n",
    "                    '[class*=\"location\"]',\n",
    "                    '[class*=\"companyLocation\"]'\n",
    "                ]\n",
    "                for selector in location_selectors:\n",
    "                    try:\n",
    "                        location_elem = card.find_element(By.CSS_SELECTOR, selector)\n",
    "                        location = location_elem.text.strip()\n",
    "                        if location:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Salary\n",
    "                salary = \"Salary not specified\"\n",
    "                salary_selectors = [\n",
    "                    '[data-testid=\"attribute_snippet_testid\"]',\n",
    "                    '[class*=\"salary\"]',\n",
    "                    '[class*=\"compensation\"]'\n",
    "                ]\n",
    "                for selector in salary_selectors:\n",
    "                    try:\n",
    "                        salary_elem = card.find_element(By.CSS_SELECTOR, selector)\n",
    "                        salary_text = salary_elem.text.strip()\n",
    "                        if salary_text and '$' in salary_text:\n",
    "                            salary = salary_text\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Link\n",
    "                link = \"Link not available\"\n",
    "                try:\n",
    "                    link_elem = card.find_element(By.CSS_SELECTOR, 'h2.jobTitle a')\n",
    "                    link = link_elem.get_attribute('href')\n",
    "                    if link and link.startswith('/'):\n",
    "                        link = 'https://ca.indeed.com' + link\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                job_info = {\n",
    "                    'title': title,\n",
    "                    'company': company,\n",
    "                    'location': location,\n",
    "                    'salary': salary,\n",
    "                    'link': link,\n",
    "                    'scraped_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "                jobs_data.append(job_info)\n",
    "                print(f\"{i+1}. {title[:40]}... | {company} | {location}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with card {i+1}: {e}\")\n",
    "        \n",
    "        return jobs_data\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Final Indeed Canada Job Scraper...\")\n",
    "    \n",
    "    # Try the CSS selector approach first\n",
    "    print(\"=== TRYING CSS SELECTOR APPROACH ===\")\n",
    "    css_jobs = scrape_with_css_selectors()\n",
    "    \n",
    "    if css_jobs:\n",
    "        df = pd.DataFrame(css_jobs)\n",
    "        df.to_csv(\"css_selector_jobs.csv\", index=False)\n",
    "        print(f\"CSS approach saved {len(css_jobs)} jobs\")\n",
    "        \n",
    "        print(\"\\nCSS Selector Results:\")\n",
    "        for i, job in enumerate(css_jobs[:3], 1):\n",
    "            print(f\"{i}. {job['title']}\")\n",
    "            print(f\"   Company: {job['company']}\")\n",
    "            print(f\"   Location: {job['location']}\")\n",
    "            print(f\"   Salary: {job['salary']}\")\n",
    "            print()\n",
    "    \n",
    "    print(\"\\n=== TRYING PRECISE TEXT ANALYSIS APPROACH ===\")\n",
    "    scraper = IndeedCanadaScraper(headless=False)\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=\"developer\",\n",
    "        location=\"Canada\",\n",
    "        max_pages=1\n",
    "    )\n",
    "    \n",
    "    if jobs:\n",
    "        scraper.save_to_csv(\"precise_analysis_jobs.csv\")\n",
    "        print(f\"\\nPrecise analysis saved {len(jobs)} jobs\")\n",
    "        \n",
    "        print(\"\\nPrecise Analysis Results:\")\n",
    "        for i, job in enumerate(jobs[:3], 1):\n",
    "            print(f\"{i}. {job['title']}\")\n",
    "            print(f\"   Company: {job['company']}\")\n",
    "            print(f\"   Location: {job['location']}\")\n",
    "            print(f\"   Salary: {job['salary']}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551abb5c-9053-41f6-8dc8-134c7ac9fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Indeed Canada Job Scraper - Final Version\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job title (default: developer):  developer\n",
      "Enter location (default: Canada):  canada\n",
      "Enter number of pages to scrape (default: 3):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping 'developer' jobs in 'canada' (5 pages)...\n",
      "Scraping page 1...\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 1\n",
      "Scraping page 2...\n",
      "No job cards found on page 2\n",
      "Scraping page 3...\n",
      "No job cards found on page 3\n",
      "Scraping page 4...\n",
      "No job cards found on page 4\n",
      "Scraping page 5...\n",
      "No job cards found on page 5\n",
      "\n",
      "âœ… Successfully scraped 15 jobs in 72.1 seconds\n",
      "Successfully saved 15 jobs to indeed_developer_canada_jobs.csv\n",
      "\n",
      "================================================================================\n",
      "SAMPLE OF 5 SCRAPED JOBS\n",
      "================================================================================\n",
      "\n",
      "1. Wireless DSP Software Developer\n",
      "   Company: Skycope C-UAS Technologies Inc\n",
      "   Location: Burnaby, BC V5G 1J9\n",
      "   Salary: $100,000â€“$150,000 a year\n",
      "   Type: Full-time\n",
      "   Link: https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0A7000hUPCMxGwW2vqsju70Hf0EmrA...\n",
      "\n",
      "2. Intermediate Full Stack Software Engineer\n",
      "   Company: D3 Security Management Systems\n",
      "   Location: Vancouver, BC V6C 1G8\n",
      "   Salary: $70,000â€“$100,000 a year\n",
      "   Type: Full-time\n",
      "   Link: https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BvCuWiBIaNCSUz0Q4GXryJc0qOCu-...\n",
      "\n",
      "3. CNC Programmer/Operator\n",
      "   Company: Winters Technical Staffing\n",
      "   Location: Concord, ON L4K 2C1\n",
      "   Salary: $60,000â€“$65,000 a year\n",
      "   Type: Full-time\n",
      "   Link: https://ca.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0ATrnH-B4M6mbej6oULLijvNdE0W00...\n",
      "\n",
      "4. administrative assistant\n",
      "   Company: Majha Developers Limited\n",
      "   Location: Brampton, ON\n",
      "   Salary: $30.00â€“$36.25 an hour\n",
      "   Type: Full-time\n",
      "   Link: https://ca.indeed.com/rc/clk?jk=636b9b4b16400d4d&bb=Vt0a9kd-lIp8iIiyzEUln-m3hldt...\n",
      "\n",
      "5. Email HTML Developer\n",
      "   Company: Wired Messenger Inc\n",
      "   Location: Remote\n",
      "   Salary: Salary not specified\n",
      "   Type: Full-time\n",
      "   Link: https://ca.indeed.com/rc/clk?jk=c80fc365d2ca9d29&bb=Vt0a9kd-lIp8iIiyzEUln1EOe20L...\n",
      "\n",
      "ðŸ“Š STATISTICS:\n",
      "   â€¢ Total jobs: 15\n",
      "   â€¢ Unique companies: 15\n",
      "   â€¢ Locations: 15\n",
      "   â€¢ Remote jobs: 0\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "class IndeedCanadaScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "    \n",
    "    def scrape_jobs(self, job_title=\"developer\", location=\"Canada\", max_pages=3):\n",
    "        \"\"\"Scrape jobs using reliable CSS selectors\"\"\"\n",
    "        \n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"Scraping page {page + 1}...\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_css()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"Added {jobs_added} jobs from page {page + 1}\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _extract_page_jobs_css(self):\n",
    "        \"\"\"Extract jobs using reliable CSS selectors\"\"\"\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for card in job_cards:\n",
    "                job_info = self._extract_job_info_css(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_css(self, card):\n",
    "        \"\"\"Extract job information using CSS selectors\"\"\"\n",
    "        try:\n",
    "            # Title\n",
    "            title = self._safe_extract(card, 'h2.jobTitle', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            # Company - try multiple selectors\n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"company\"]', 'text')\n",
    "            \n",
    "            # Location - try multiple selectors\n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Salary\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_selectors = [\n",
    "                '[data-testid=\"attribute_snippet_testid\"]',\n",
    "                '[class*=\"salary\"]',\n",
    "                '[class*=\"compensation\"]',\n",
    "                '.salary-snippet'\n",
    "            ]\n",
    "            for selector in salary_selectors:\n",
    "                salary_text = self._safe_extract(card, selector, 'text')\n",
    "                if salary_text and '$' in salary_text:\n",
    "                    salary = salary_text\n",
    "                    break\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if not link:\n",
    "                link = self._safe_extract(card, 'a.jcs-JobTitle', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type (bonus field)\n",
    "            job_type = self._extract_job_type(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'scraped_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        \"\"\"Safely extract element text or attribute\"\"\"\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _extract_job_type(self, card):\n",
    "        \"\"\"Extract job type if available\"\"\"\n",
    "        try:\n",
    "            # Look for job type indicators in the card text\n",
    "            card_text = card.text.lower()\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def save_to_csv(self, filename=\"indeed_canada_jobs_final.csv\"):\n",
    "        \"\"\"Save results to CSV\"\"\"\n",
    "        if not self.jobs_data:\n",
    "            print(\"No jobs data to save.\")\n",
    "            return False\n",
    "        \n",
    "        df = pd.DataFrame(self.jobs_data)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Successfully saved {len(self.jobs_data)} jobs to {filename}\")\n",
    "        return True\n",
    "\n",
    "    def display_sample(self, count=5):\n",
    "        \"\"\"Display sample of scraped jobs\"\"\"\n",
    "        if not self.jobs_data:\n",
    "            print(\"No jobs to display.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SAMPLE OF {min(count, len(self.jobs_data))} SCRAPED JOBS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, job in enumerate(self.jobs_data[:count], 1):\n",
    "            print(f\"\\n{i}. {job['title']}\")\n",
    "            print(f\"   Company: {job['company']}\")\n",
    "            print(f\"   Location: {job['location']}\")\n",
    "            print(f\"   Salary: {job['salary']}\")\n",
    "            print(f\"   Type: {job['job_type']}\")\n",
    "            print(f\"   Link: {job['link'][:80]}...\")\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ Indeed Canada Job Scraper - Final Version\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = input(\"Enter job title (default: developer): \").strip() or \"developer\"\n",
    "    location = input(\"Enter location (default: Canada): \").strip() or \"Canada\"\n",
    "    pages = input(\"Enter number of pages to scrape (default: 3): \").strip()\n",
    "    pages = int(pages) if pages.isdigit() else 3\n",
    "    \n",
    "    print(f\"\\nScraping '{job_title}' jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Initialize and run scraper\n",
    "    scraper = IndeedCanadaScraper(headless=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nâœ… Successfully scraped {len(jobs)} jobs in {end_time - start_time:.1f} seconds\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        filename = f\"indeed_{job_title.replace(' ', '_')}_{location.replace(' ', '_')}_jobs.csv\"\n",
    "        scraper.save_to_csv(filename)\n",
    "        \n",
    "        # Display sample\n",
    "        scraper.display_sample(5)\n",
    "        \n",
    "        # Show statistics\n",
    "        companies = set(job['company'] for job in jobs if job['company'] != \"Company not found\")\n",
    "        locations = set(job['location'] for job in jobs if job['location'] != \"Location not specified\")\n",
    "        remote_jobs = sum(1 for job in jobs if 'remote' in job['job_type'].lower())\n",
    "        \n",
    "        print(f\"\\nðŸ“Š STATISTICS:\")\n",
    "        print(f\"   â€¢ Total jobs: {len(jobs)}\")\n",
    "        print(f\"   â€¢ Unique companies: {len(companies)}\")\n",
    "        print(f\"   â€¢ Locations: {len(locations)}\")\n",
    "        print(f\"   â€¢ Remote jobs: {remote_jobs}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No jobs were scraped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c621c1ac-4a99-4f91-92c3-444373492e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Indeed Scraper with H2 Database\n",
      "==================================================\n",
      "âŒ Error setting up H2 database: Class org.h2.Driver is not found\n",
      "Failed to setup database. Exiting.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import jaydebeapi\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class IndeedScraperWithH2:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "        self.db_connection = None\n",
    "        \n",
    "    def setup_h2_database(self, db_path=\"indeed_jobs.db\"):\n",
    "        \"\"\"Setup H2 database connection\"\"\"\n",
    "        try:\n",
    "            # H2 JDBC driver class and connection string\n",
    "            jdbc_driver = 'org.h2.Driver'\n",
    "            jdbc_url = f'jdbc:h2:file:./{db_path}'\n",
    "            \n",
    "            # Connect to H2 database (it will create if doesn't exist)\n",
    "            self.db_connection = jaydebeapi.connect(\n",
    "                jdbc_driver,\n",
    "                jdbc_url,\n",
    "                ['sa', '']  # username, password\n",
    "            )\n",
    "            \n",
    "            # Create jobs table if it doesn't exist\n",
    "            self._create_jobs_table()\n",
    "            print(f\"âœ… H2 database connected: {db_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error setting up H2 database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_jobs_table(self):\n",
    "        \"\"\"Create the jobs table if it doesn't exist\"\"\"\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id BIGINT AUTO_INCREMENT PRIMARY KEY,\n",
    "            title VARCHAR(500) NOT NULL,\n",
    "            company VARCHAR(255),\n",
    "            location VARCHAR(255),\n",
    "            salary VARCHAR(100),\n",
    "            job_type VARCHAR(50),\n",
    "            link VARCHAR(1000),\n",
    "            scraped_date TIMESTAMP,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(title, company, location)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = self.db_connection.cursor()\n",
    "        cursor.execute(create_table_sql)\n",
    "        cursor.close()\n",
    "        print(\"âœ… Jobs table created/verified\")\n",
    "\n",
    "    def scrape_jobs(self, job_title=\"developer\", location=\"Canada\", max_pages=3):\n",
    "        \"\"\"Scrape jobs and automatically save to H2 database\"\"\"\n",
    "        \n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"Scraping page {page + 1}...\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_css()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"Added {jobs_added} jobs from page {page + 1}\")\n",
    "                \n",
    "                # Save to database after each page\n",
    "                if jobs_added > 0:\n",
    "                    saved_count = self.save_to_database()\n",
    "                    print(f\"ðŸ’¾ Saved {saved_count} jobs to database\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()\n",
    "                print(\"âœ… Database connection closed\")\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _extract_page_jobs_css(self):\n",
    "        \"\"\"Extract jobs using reliable CSS selectors\"\"\"\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for card in job_cards:\n",
    "                job_info = self._extract_job_info_css(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_css(self, card):\n",
    "        \"\"\"Extract job information using CSS selectors\"\"\"\n",
    "        try:\n",
    "            # Title\n",
    "            title = self._safe_extract(card, 'h2.jobTitle', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            # Company\n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            \n",
    "            # Location\n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Salary\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_text = self._safe_extract(card, '[data-testid=\"attribute_snippet_testid\"]', 'text')\n",
    "            if salary_text and '$' in salary_text:\n",
    "                salary = salary_text\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type\n",
    "            job_type = self._extract_job_type(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'scraped_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        \"\"\"Safely extract element text or attribute\"\"\"\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _extract_job_type(self, card):\n",
    "        \"\"\"Extract job type\"\"\"\n",
    "        try:\n",
    "            card_text = card.text.lower()\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def save_to_database(self):\n",
    "        \"\"\"Save current jobs data to H2 database\"\"\"\n",
    "        if not self.jobs_data or not self.db_connection:\n",
    "            print(\"No data to save or database not connected\")\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            saved_count = 0\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO jobs (title, company, location, salary, job_type, link, scraped_date)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            for job in self.jobs_data:\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (\n",
    "                        job['title'],\n",
    "                        job['company'],\n",
    "                        job['location'],\n",
    "                        job['salary'],\n",
    "                        job['job_type'],\n",
    "                        job['link'],\n",
    "                        job['scraped_date']\n",
    "                    ))\n",
    "                    saved_count += 1\n",
    "                except Exception as e:\n",
    "                    # This might be a duplicate, which is fine due to UNIQUE constraint\n",
    "                    continue\n",
    "            \n",
    "            self.db_connection.commit()\n",
    "            cursor.close()\n",
    "            return saved_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to database: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def query_jobs_from_database(self, limit=10):\n",
    "        \"\"\"Query jobs from H2 database\"\"\"\n",
    "        if not self.db_connection:\n",
    "            print(\"Database not connected\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT title, company, location, salary, job_type, link, scraped_date \n",
    "                FROM jobs \n",
    "                ORDER BY scraped_date DESC \n",
    "                LIMIT ?\n",
    "            \"\"\", (limit,))\n",
    "            \n",
    "            results = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            \n",
    "            jobs = []\n",
    "            for row in results:\n",
    "                jobs.append({\n",
    "                    'title': row[0],\n",
    "                    'company': row[1],\n",
    "                    'location': row[2],\n",
    "                    'salary': row[3],\n",
    "                    'job_type': row[4],\n",
    "                    'link': row[5],\n",
    "                    'scraped_date': row[6]\n",
    "                })\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error querying database: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_database_stats(self):\n",
    "        \"\"\"Get statistics from the database\"\"\"\n",
    "        if not self.db_connection:\n",
    "            print(\"Database not connected\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            \n",
    "            # Total jobs count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "            total_jobs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Companies count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT company) FROM jobs\")\n",
    "            unique_companies = cursor.fetchone()[0]\n",
    "            \n",
    "            # Locations count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT location) FROM jobs\")\n",
    "            unique_locations = cursor.fetchone()[0]\n",
    "            \n",
    "            # Latest scrape date\n",
    "            cursor.execute(\"SELECT MAX(scraped_date) FROM jobs\")\n",
    "            latest_scrape = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.close()\n",
    "            \n",
    "            return {\n",
    "                'total_jobs': total_jobs,\n",
    "                'unique_companies': unique_companies,\n",
    "                'unique_locations': unique_locations,\n",
    "                'latest_scrape': latest_scrape\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting database stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def save_to_csv(self, filename=\"indeed_jobs_backup.csv\"):\n",
    "        \"\"\"Backup database to CSV file\"\"\"\n",
    "        if not self.db_connection:\n",
    "            print(\"Database not connected\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            cursor.execute(\"SELECT * FROM jobs\")\n",
    "            \n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "            cursor.close()\n",
    "            print(f\"ðŸ’¾ Database backed up to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error backing up to CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ Indeed Scraper with H2 Database\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize scraper with H2 database\n",
    "    scraper = IndeedScraperWithH2(headless=True)\n",
    "    \n",
    "    # Setup H2 database\n",
    "    if not scraper.setup_h2_database(\"indeed_jobs_db\"):\n",
    "        print(\"Failed to setup database. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = input(\"Enter job title (default: developer): \").strip() or \"developer\"\n",
    "    location = input(\"Enter location (default: Canada): \").strip() or \"Canada\"\n",
    "    pages = input(\"Enter number of pages to scrape (default: 2): \").strip()\n",
    "    pages = int(pages) if pages.isdigit() else 2\n",
    "    \n",
    "    print(f\"\\nScraping '{job_title}' jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Scrape and save to database\n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nâœ… Successfully processed {len(jobs)} jobs in {end_time - start_time:.1f} seconds\")\n",
    "        \n",
    "        # Get database statistics\n",
    "        stats = scraper.get_database_stats()\n",
    "        print(f\"\\nðŸ“Š DATABASE STATISTICS:\")\n",
    "        print(f\"   â€¢ Total jobs in database: {stats.get('total_jobs', 0)}\")\n",
    "        print(f\"   â€¢ Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "        print(f\"   â€¢ Unique locations: {stats.get('unique_locations', 0)}\")\n",
    "        print(f\"   â€¢ Latest scrape: {stats.get('latest_scrape', 'N/A')}\")\n",
    "        \n",
    "        # Query and display recent jobs from database\n",
    "        recent_jobs = scraper.query_jobs_from_database(limit=5)\n",
    "        if recent_jobs:\n",
    "            print(f\"\\nðŸ“‹ RECENT JOBS FROM DATABASE:\")\n",
    "            for i, job in enumerate(recent_jobs, 1):\n",
    "                print(f\"\\n{i}. {job['title']}\")\n",
    "                print(f\"   Company: {job['company']}\")\n",
    "                print(f\"   Location: {job['location']}\")\n",
    "                print(f\"   Salary: {job['salary']}\")\n",
    "                print(f\"   Scraped: {job['scraped_date']}\")\n",
    "        \n",
    "        # Backup to CSV\n",
    "        scraper.save_to_csv(\"indeed_jobs_backup.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No jobs were scraped\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Database file: indeed_jobs_db.mv.db\")\n",
    "    print(\"ðŸ”§ You can query the database using any SQL client that supports H2\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d892170-88ee-46c7-b830-8f92c42402e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jaydebeapi\n",
      "  Downloading JayDeBeApi-1.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting JPype1 (from jaydebeapi)\n",
      "  Downloading jpype1-1.6.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lesha\\anaconda3\\lib\\site-packages (from JPype1->jaydebeapi) (24.1)\n",
      "Downloading JayDeBeApi-1.2.3-py3-none-any.whl (26 kB)\n",
      "Downloading jpype1-1.6.0-cp312-cp312-win_amd64.whl (355 kB)\n",
      "Installing collected packages: JPype1, jaydebeapi\n",
      "Successfully installed JPype1-1.6.0 jaydebeapi-1.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jaydebeapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d09ad5f-b981-4264-8078-fa705d617e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Setting up H2 database environment...\n",
      "âœ… jaydebeapi already installed\n",
      "ðŸ“¦ Installing JPype1...\n",
      "âœ… selenium already installed\n",
      "âœ… pandas already installed\n",
      "âœ… webdriver-manager already installed\n",
      "âœ… requests already installed\n",
      "ðŸ“¥ Downloading H2 database driver...\n",
      "âœ… H2 driver downloaded successfully\n",
      "âœ… Setup complete! Now run the main scraper.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_h2_driver():\n",
    "    \"\"\"Download H2 database driver if not present\"\"\"\n",
    "    h2_jar_path = \"h2-2.2.224.jar\"\n",
    "    h2_url = \"https://repo1.maven.org/maven2/com/h2database/h2/2.2.224/h2-2.2.224.jar\"\n",
    "    \n",
    "    if not os.path.exists(h2_jar_path):\n",
    "        print(\"ðŸ“¥ Downloading H2 database driver...\")\n",
    "        try:\n",
    "            response = requests.get(h2_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(h2_jar_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(\"âœ… H2 driver downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error downloading H2 driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    \"\"\"Install required Python packages\"\"\"\n",
    "    packages = ['jaydebeapi', 'JPype1', 'selenium', 'pandas', 'webdriver-manager', 'requests']\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"âœ… {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"ðŸ“¦ Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Run setup\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Setting up H2 database environment...\")\n",
    "    install_packages()\n",
    "    setup_h2_driver()\n",
    "    print(\"âœ… Setup complete! Now run the main scraper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e48d55-af87-4807-bc77-04fb967850cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Indeed Scraper with H2 Database\n",
      "==================================================\n",
      "ðŸ”— Connecting to H2 database: ./indeed_jobs_db\n",
      "âŒ Error setting up H2 database: Class org.h2.Driver is not found\n",
      "âŒ Failed to setup H2 database. Exiting.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import jaydebeapi\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class IndeedScraperWithH2:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "        self.db_connection = None\n",
    "        self.h2_jar_path = \"h2-2.2.224.jar\"\n",
    "        \n",
    "    def setup_h2_database(self, db_path=\"./indeed_jobs\"):\n",
    "        \"\"\"Setup H2 database connection with proper JAR path\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.h2_jar_path):\n",
    "                print(\"âŒ H2 driver JAR not found. Please run setup script first.\")\n",
    "                return False\n",
    "            \n",
    "            # H2 JDBC connection details\n",
    "            jdbc_driver = \"org.h2.Driver\"\n",
    "            jdbc_url = f\"jdbc:h2:file:{db_path}\"\n",
    "            \n",
    "            print(f\"ðŸ”— Connecting to H2 database: {db_path}\")\n",
    "            \n",
    "            # Connect to H2 database\n",
    "            self.db_connection = jaydebeapi.connect(\n",
    "                jdbc_driver,\n",
    "                jdbc_url,\n",
    "                [\"sa\", \"\"],  # username, password\n",
    "                self.h2_jar_path\n",
    "            )\n",
    "            \n",
    "            # Create jobs table\n",
    "            self._create_jobs_table()\n",
    "            print(\"âœ… H2 database connected and ready\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error setting up H2 database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_jobs_table(self):\n",
    "        \"\"\"Create the jobs table in H2 database\"\"\"\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id BIGINT AUTO_INCREMENT PRIMARY KEY,\n",
    "            title VARCHAR(500) NOT NULL,\n",
    "            company VARCHAR(255),\n",
    "            location VARCHAR(255),\n",
    "            salary VARCHAR(100),\n",
    "            job_type VARCHAR(50),\n",
    "            link VARCHAR(1000),\n",
    "            scraped_date TIMESTAMP,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(title, company, location)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = self.db_connection.cursor()\n",
    "        cursor.execute(create_table_sql)\n",
    "        cursor.close()\n",
    "        self.db_connection.commit()\n",
    "        print(\"âœ… Jobs table created/verified in H2 database\")\n",
    "\n",
    "    def scrape_jobs(self, job_title=\"developer\", location=\"Canada\", max_pages=3):\n",
    "        \"\"\"Scrape jobs and save to H2 database\"\"\"\n",
    "        \n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"ðŸŒ Scraping page {page + 1}...\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"âŒ No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_css()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"âœ… Added {jobs_added} jobs from page {page + 1}\")\n",
    "                \n",
    "                # Save to H2 database after each page\n",
    "                if jobs_added > 0 and self.db_connection:\n",
    "                    saved_count = self.save_to_h2_database()\n",
    "                    print(f\"ðŸ’¾ Saved {saved_count} jobs to H2 database\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()\n",
    "                print(\"âœ… H2 database connection closed\")\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _extract_page_jobs_css(self):\n",
    "        \"\"\"Extract jobs from current page\"\"\"\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"ðŸ“„ Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for card in job_cards:\n",
    "                job_info = self._extract_job_info_css(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_css(self, card):\n",
    "        \"\"\"Extract job information from card\"\"\"\n",
    "        try:\n",
    "            # Title\n",
    "            title = self._safe_extract(card, 'h2.jobTitle', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            # Company\n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            \n",
    "            # Location\n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Salary\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_text = self._safe_extract(card, '[data-testid=\"attribute_snippet_testid\"]', 'text')\n",
    "            if salary_text and '$' in salary_text:\n",
    "                salary = salary_text\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type\n",
    "            job_type = self._extract_job_type(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'scraped_date': datetime.now()\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        \"\"\"Safely extract element text or attribute\"\"\"\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _extract_job_type(self, card):\n",
    "        \"\"\"Extract job type from card\"\"\"\n",
    "        try:\n",
    "            card_text = card.text.lower()\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def save_to_h2_database(self):\n",
    "        \"\"\"Save jobs data to H2 database\"\"\"\n",
    "        if not self.jobs_data or not self.db_connection:\n",
    "            print(\"âŒ No data to save or database not connected\")\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            saved_count = 0\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            MERGE INTO jobs (title, company, location, salary, job_type, link, scraped_date)\n",
    "            KEY(title, company, location)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            for job in self.jobs_data:\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (\n",
    "                        job['title'],\n",
    "                        job['company'],\n",
    "                        job['location'],\n",
    "                        job['salary'],\n",
    "                        job['job_type'],\n",
    "                        job['link'],\n",
    "                        job['scraped_date']\n",
    "                    ))\n",
    "                    saved_count += 1\n",
    "                except Exception as e:\n",
    "                    # Duplicate entry, skip\n",
    "                    continue\n",
    "            \n",
    "            self.db_connection.commit()\n",
    "            cursor.close()\n",
    "            return saved_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving to H2 database: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def query_h2_database(self, limit=10):\n",
    "        \"\"\"Query jobs from H2 database\"\"\"\n",
    "        if not self.db_connection:\n",
    "            print(\"âŒ Database not connected\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT title, company, location, salary, job_type, link, scraped_date \n",
    "                FROM jobs \n",
    "                ORDER BY scraped_date DESC \n",
    "                LIMIT ?\n",
    "            \"\"\", (limit,))\n",
    "            \n",
    "            results = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            \n",
    "            jobs = []\n",
    "            for row in results:\n",
    "                jobs.append({\n",
    "                    'title': row[0],\n",
    "                    'company': row[1],\n",
    "                    'location': row[2],\n",
    "                    'salary': row[3],\n",
    "                    'job_type': row[4],\n",
    "                    'link': row[5],\n",
    "                    'scraped_date': row[6]\n",
    "                })\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error querying H2 database: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_h2_database_stats(self):\n",
    "        \"\"\"Get statistics from H2 database\"\"\"\n",
    "        if not self.db_connection:\n",
    "            print(\"âŒ Database not connected\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            \n",
    "            # Total jobs count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "            total_jobs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Companies count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT company) FROM jobs\")\n",
    "            unique_companies = cursor.fetchone()[0]\n",
    "            \n",
    "            # Locations count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT location) FROM jobs\")\n",
    "            unique_locations = cursor.fetchone()[0]\n",
    "            \n",
    "            # Latest scrape date\n",
    "            cursor.execute(\"SELECT MAX(scraped_date) FROM jobs\")\n",
    "            latest_scrape = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.close()\n",
    "            \n",
    "            return {\n",
    "                'total_jobs': total_jobs,\n",
    "                'unique_companies': unique_companies,\n",
    "                'unique_locations': unique_locations,\n",
    "                'latest_scrape': latest_scrape\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error getting H2 database stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def export_h2_to_csv(self, filename=\"indeed_jobs_h2_backup.csv\"):\n",
    "        \"\"\"Export H2 database to CSV\"\"\"\n",
    "        if not self.db_connection:\n",
    "            print(\"âŒ Database not connected\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            cursor.execute(\"SELECT * FROM jobs\")\n",
    "            \n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "            cursor.close()\n",
    "            print(f\"ðŸ’¾ H2 database backed up to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error backing up H2 to CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ Indeed Scraper with H2 Database\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if H2 driver exists\n",
    "    if not os.path.exists(\"h2-2.2.224.jar\"):\n",
    "        print(\"âŒ H2 driver not found. Please run the setup script first.\")\n",
    "        print(\"ðŸ’¡ Run: python setup_h2.py\")\n",
    "        return\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = IndeedScraperWithH2(headless=True)\n",
    "    \n",
    "    # Setup H2 database\n",
    "    if not scraper.setup_h2_database(\"./indeed_jobs_db\"):\n",
    "        print(\"âŒ Failed to setup H2 database. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = input(\"Enter job title (default: developer): \").strip() or \"developer\"\n",
    "    location = input(\"Enter location (default: Canada): \").strip() or \"Canada\"\n",
    "    pages = input(\"Enter number of pages to scrape (default: 2): \").strip()\n",
    "    pages = int(pages) if pages.isdigit() else 2\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Scraping '{job_title}' jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Scrape and save to H2 database\n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nâœ… Successfully processed {len(jobs)} jobs in {end_time - start_time:.1f} seconds\")\n",
    "        \n",
    "        # Get H2 database statistics\n",
    "        stats = scraper.get_h2_database_stats()\n",
    "        print(f\"\\nðŸ“Š H2 DATABASE STATISTICS:\")\n",
    "        print(f\"   â€¢ Total jobs in H2: {stats.get('total_jobs', 0)}\")\n",
    "        print(f\"   â€¢ Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "        print(f\"   â€¢ Unique locations: {stats.get('unique_locations', 0)}\")\n",
    "        print(f\"   â€¢ Latest scrape: {stats.get('latest_scrape', 'N/A')}\")\n",
    "        \n",
    "        # Query and display recent jobs from H2 database\n",
    "        recent_jobs = scraper.query_h2_database(limit=5)\n",
    "        if recent_jobs:\n",
    "            print(f\"\\nðŸ“‹ RECENT JOBS FROM H2 DATABASE:\")\n",
    "            for i, job in enumerate(recent_jobs, 1):\n",
    "                print(f\"\\n{i}. {job['title']}\")\n",
    "                print(f\"   Company: {job['company']}\")\n",
    "                print(f\"   Location: {job['location']}\")\n",
    "                print(f\"   Salary: {job['salary']}\")\n",
    "                print(f\"   Scraped: {job['scraped_date']}\")\n",
    "        \n",
    "        # Backup H2 to CSV\n",
    "        scraper.export_h2_to_csv(\"indeed_jobs_h2_backup.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No jobs were scraped\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ H2 database files:\")\n",
    "    print(f\"   - indeed_jobs_db.mv.db (main database file)\")\n",
    "    print(f\"   - indeed_jobs_db.trace.db (log file)\")\n",
    "    print(f\"   - indeed_jobs_h2_backup.csv (CSV backup)\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ You can connect to H2 database using:\")\n",
    "    print(f\"   JDBC URL: jdbc:h2:file:./indeed_jobs_db\")\n",
    "    print(f\"   Username: sa\")\n",
    "    print(f\"   Password: (empty)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13a8ce5-11f9-44a2-ba24-bbbbe4851b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Setting up H2 Database Environment...\n",
      "âœ… jaydebeapi already installed\n",
      "ðŸ“¦ Installing JPype1==1.4.1...\n",
      "âš ï¸  Failed to install JPype1==1.4.1: Command '['C:\\\\Users\\\\lesha\\\\anaconda3\\\\python.exe', '-m', 'pip', 'install', 'JPype1==1.4.1']' returned non-zero exit status 1.\n",
      "âœ… selenium already installed\n",
      "âœ… pandas already installed\n",
      "âœ… webdriver-manager already installed\n",
      "âœ… requests already installed\n",
      "âœ… H2 driver already exists: h2-2.2.224.jar\n",
      "\n",
      "âœ… Setup complete!\n",
      "ðŸ“ H2 JAR file: h2-2.2.224.jar\n",
      "ðŸ”§ Now run: python indeed_scraper_h2.py\n"
     ]
    }
   ],
   "source": [
    "# setup_h2.py\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def download_h2_jar():\n",
    "    \"\"\"Download H2 database JAR file\"\"\"\n",
    "    h2_version = \"2.2.224\"\n",
    "    h2_jar_name = f\"h2-{h2_version}.jar\"\n",
    "    h2_url = f\"https://repo1.maven.org/maven2/com/h2database/h2/{h2_version}/{h2_jar_name}\"\n",
    "    \n",
    "    if not os.path.exists(h2_jar_name):\n",
    "        print(f\"ðŸ“¥ Downloading H2 database {h2_version}...\")\n",
    "        try:\n",
    "            response = requests.get(h2_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(h2_jar_name, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"âœ… H2 driver downloaded: {h2_jar_name}\")\n",
    "            return h2_jar_name\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error downloading H2: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"âœ… H2 driver already exists: {h2_jar_name}\")\n",
    "        return h2_jar_name\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Install required Python packages\"\"\"\n",
    "    requirements = [\n",
    "        'jaydebeapi',\n",
    "        'JPype1==1.4.1',\n",
    "        'selenium',\n",
    "        'pandas',\n",
    "        'webdriver-manager',\n",
    "        'requests'\n",
    "    ]\n",
    "    \n",
    "    for package in requirements:\n",
    "        try:\n",
    "            if '==' in package:\n",
    "                package_name = package.split('==')[0]\n",
    "            else:\n",
    "                package_name = package\n",
    "                \n",
    "            __import__(package_name.replace('-', '_'))\n",
    "            print(f\"âœ… {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"ðŸ“¦ Installing {package}...\")\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"âš ï¸  Failed to install {package}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Setting up H2 Database Environment...\")\n",
    "    install_requirements()\n",
    "    jar_file = download_h2_jar()\n",
    "    \n",
    "    if jar_file:\n",
    "        print(f\"\\nâœ… Setup complete!\")\n",
    "        print(f\"ðŸ“ H2 JAR file: {jar_file}\")\n",
    "        print(f\"ðŸ”§ Now run: python indeed_scraper_h2.py\")\n",
    "    else:\n",
    "        print(\"âŒ Setup failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ef29f8-4e41-46c4-8439-a127d47c631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Indeed Scraper with H2 Database\n",
      "==================================================\n",
      "ðŸ”§ Using H2 JAR: h2-2.2.224.jar\n",
      "ðŸ”— Connecting to H2 database: jdbc:h2:file:./indeed_jobs_db;DB_CLOSE_DELAY=-1\n",
      "âŒ Error setting up H2 database: Class org.h2.Driver is not found\n",
      "âŒ Failed to setup H2 database.\n",
      "ðŸ’¡ Make sure you ran: python setup_h2.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lesha\\AppData\\Local\\Temp\\ipykernel_24228\\700885912.py\", line 57, in setup_h2_database\n",
      "    self.db_connection = jaydebeapi.connect(\n",
      "                         ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\lesha\\anaconda3\\Lib\\site-packages\\jaydebeapi\\__init__.py\", line 412, in connect\n",
      "    jconn = _jdbc_connect(jclassname, url, driver_args, jars, libs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\lesha\\anaconda3\\Lib\\site-packages\\jaydebeapi\\__init__.py\", line 221, in _jdbc_connect_jpype\n",
      "    jpype.JClass(jclassname)\n",
      "  File \"C:\\Users\\lesha\\anaconda3\\Lib\\site-packages\\jpype\\_jclass.py\", line 99, in __new__\n",
      "    return _jpype._getClass(jc)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Class org.h2.Driver is not found\n"
     ]
    }
   ],
   "source": [
    "# indeed_scraper_h2.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import jaydebeapi\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class IndeedScraperWithH2:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "        self.db_connection = None\n",
    "        \n",
    "    def setup_h2_database(self, db_name=\"indeed_jobs\"):\n",
    "        \"\"\"Setup H2 database connection with proper JAR loading\"\"\"\n",
    "        try:\n",
    "            # Find H2 JAR file\n",
    "            h2_jar = self._find_h2_jar()\n",
    "            if not h2_jar:\n",
    "                print(\"âŒ H2 JAR file not found. Please run setup_h2.py first.\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"ðŸ”§ Using H2 JAR: {h2_jar}\")\n",
    "            \n",
    "            # H2 JDBC connection details\n",
    "            jdbc_driver = \"org.h2.Driver\"\n",
    "            jdbc_url = f\"jdbc:h2:file:./{db_name};DB_CLOSE_DELAY=-1\"\n",
    "            \n",
    "            print(f\"ðŸ”— Connecting to H2 database: {jdbc_url}\")\n",
    "            \n",
    "            # Connect to H2 database with explicit JAR path\n",
    "            self.db_connection = jaydebeapi.connect(\n",
    "                jdbc_driver,\n",
    "                jdbc_url,\n",
    "                [\"sa\", \"\"],  # username, password\n",
    "                h2_jar\n",
    "            )\n",
    "            \n",
    "            # Create jobs table\n",
    "            self._create_jobs_table()\n",
    "            print(\"âœ… H2 database connected and ready\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error setting up H2 database: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def _find_h2_jar(self):\n",
    "        \"\"\"Find the H2 JAR file in current directory\"\"\"\n",
    "        for file in os.listdir('.'):\n",
    "            if file.startswith('h2-') and file.endswith('.jar'):\n",
    "                return file\n",
    "        return None\n",
    "\n",
    "    def _create_jobs_table(self):\n",
    "        \"\"\"Create the jobs table in H2 database\"\"\"\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id BIGINT AUTO_INCREMENT PRIMARY KEY,\n",
    "            title VARCHAR(500) NOT NULL,\n",
    "            company VARCHAR(255),\n",
    "            location VARCHAR(255),\n",
    "            salary VARCHAR(100),\n",
    "            job_type VARCHAR(50),\n",
    "            link VARCHAR(1000),\n",
    "            scraped_date TIMESTAMP,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = self.db_connection.cursor()\n",
    "        cursor.execute(create_table_sql)\n",
    "        cursor.close()\n",
    "        self.db_connection.commit()\n",
    "        print(\"âœ… Jobs table created/verified in H2 database\")\n",
    "\n",
    "    def scrape_jobs(self, job_title=\"developer\", location=\"Canada\", max_pages=3):\n",
    "        \"\"\"Scrape jobs and save to H2 database\"\"\"\n",
    "        \n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"ðŸŒ Scraping page {page + 1}...\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"âŒ No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_css()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"âœ… Added {jobs_added} jobs from page {page + 1}\")\n",
    "                \n",
    "                # Save to H2 database after each page\n",
    "                if jobs_added > 0 and self.db_connection:\n",
    "                    saved_count = self.save_to_h2_database()\n",
    "                    print(f\"ðŸ’¾ Saved {saved_count} jobs to H2 database\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during scraping: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()\n",
    "                print(\"âœ… H2 database connection closed\")\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _extract_page_jobs_css(self):\n",
    "        \"\"\"Extract jobs from current page\"\"\"\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"ðŸ“„ Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for card in job_cards:\n",
    "                job_info = self._extract_job_info_css(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_css(self, card):\n",
    "        \"\"\"Extract job information from card\"\"\"\n",
    "        try:\n",
    "            # Title\n",
    "            title = self._safe_extract(card, 'h2.jobTitle', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            # Company\n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            \n",
    "            # Location\n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Salary\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_text = self._safe_extract(card, '[data-testid=\"attribute_snippet_testid\"]', 'text')\n",
    "            if salary_text and '$' in salary_text:\n",
    "                salary = salary_text\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type\n",
    "            job_type = self._extract_job_type(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'scraped_date': datetime.now()\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        \"\"\"Safely extract element text or attribute\"\"\"\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _extract_job_type(self, card):\n",
    "        \"\"\"Extract job type from card\"\"\"\n",
    "        try:\n",
    "            card_text = card.text.lower()\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def save_to_h2_database(self):\n",
    "        \"\"\"Save jobs data to H2 database\"\"\"\n",
    "        if not self.jobs_data or not self.db_connection:\n",
    "            print(\"âŒ No data to save or database not connected\")\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            saved_count = 0\n",
    "            \n",
    "            # Use INSERT IGNORE to handle duplicates\n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO jobs (title, company, location, salary, job_type, link, scraped_date)\n",
    "            SELECT ?, ?, ?, ?, ?, ?, ?\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM jobs \n",
    "                WHERE title = ? AND company = ? AND location = ?\n",
    "            )\n",
    "            \"\"\"\n",
    "            \n",
    "            for job in self.jobs_data:\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (\n",
    "                        job['title'], job['company'], job['location'], job['salary'], \n",
    "                        job['job_type'], job['link'], job['scraped_date'],\n",
    "                        job['title'], job['company'], job['location']\n",
    "                    ))\n",
    "                    if cursor.rowcount > 0:\n",
    "                        saved_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  Error inserting job: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            self.db_connection.commit()\n",
    "            cursor.close()\n",
    "            return saved_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving to H2 database: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return 0\n",
    "\n",
    "    def get_h2_database_stats(self):\n",
    "        \"\"\"Get statistics from H2 database\"\"\"\n",
    "        if not self.db_connection:\n",
    "            # Try to reconnect to get stats\n",
    "            if not self.setup_h2_database():\n",
    "                return {}\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            \n",
    "            # Total jobs count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "            total_jobs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Companies count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT company) FROM jobs\")\n",
    "            unique_companies = cursor.fetchone()[0]\n",
    "            \n",
    "            # Locations count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT location) FROM jobs\")\n",
    "            unique_locations = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.close()\n",
    "            \n",
    "            return {\n",
    "                'total_jobs': total_jobs,\n",
    "                'unique_companies': unique_companies,\n",
    "                'unique_locations': unique_locations\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error getting H2 database stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ Indeed Scraper with H2 Database\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = IndeedScraperWithH2(headless=True)\n",
    "    \n",
    "    # Setup H2 database\n",
    "    if not scraper.setup_h2_database(\"indeed_jobs_db\"):\n",
    "        print(\"âŒ Failed to setup H2 database.\")\n",
    "        print(\"ðŸ’¡ Make sure you ran: python setup_h2.py\")\n",
    "        return\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = \"developer\"\n",
    "    location = \"Canada\" \n",
    "    pages = 2\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Scraping '{job_title}' jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Scrape and save to H2 database\n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nâœ… Successfully processed {len(jobs)} jobs in {end_time - start_time:.1f} seconds\")\n",
    "        \n",
    "        # Get H2 database statistics\n",
    "        stats = scraper.get_h2_database_stats()\n",
    "        print(f\"\\nðŸ“Š H2 DATABASE STATISTICS:\")\n",
    "        print(f\"   â€¢ Total jobs in H2: {stats.get('total_jobs', 0)}\")\n",
    "        print(f\"   â€¢ Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "        print(f\"   â€¢ Unique locations: {stats.get('unique_locations', 0)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No jobs were scraped\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ H2 database file: indeed_jobs_db.mv.db\")\n",
    "    print(\"ðŸ”§ You can use H2 Console to view the database:\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bca200f5-92e8-4d68-9a65-77ee0278d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Indeed Scraper with SQLite Database\n",
      "==================================================\n",
      "âœ… Jobs table created/verified\n",
      "âœ… SQLite database connected: indeed_jobs.sqlite\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 463\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jobs, stats\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 463\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[13], line 395\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Get user input\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m job_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter job title (default: developer): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeveloper\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter location (default: Canada): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCanada\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    397\u001b[0m pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter number of pages to scrape (default: 2): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import sqlite3  # Using SQLite instead of H2 for simplicity\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "class IndeedScraperWithDatabase:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "        self.db_connection = None\n",
    "        \n",
    "    def setup_sqlite_database(self, db_path=\"indeed_jobs.sqlite\"):\n",
    "        \"\"\"Setup SQLite database connection (no external dependencies needed)\"\"\"\n",
    "        try:\n",
    "            self.db_connection = sqlite3.connect(db_path)\n",
    "            self._create_jobs_table()\n",
    "            print(f\"âœ… SQLite database connected: {db_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error setting up SQLite database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_jobs_table(self):\n",
    "        \"\"\"Create the jobs table if it doesn't exist\"\"\"\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            company TEXT,\n",
    "            location TEXT,\n",
    "            salary TEXT,\n",
    "            job_type TEXT,\n",
    "            link TEXT,\n",
    "            scraped_date TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(title, company, location)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = self.db_connection.cursor()\n",
    "        cursor.execute(create_table_sql)\n",
    "        self.db_connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"âœ… Jobs table created/verified\")\n",
    "\n",
    "    def scrape_jobs(self, job_title=\"developer\", location=\"Canada\", max_pages=3):\n",
    "        \"\"\"Scrape jobs and automatically save to database\"\"\"\n",
    "        \n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"Scraping page {page + 1}...\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_css()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"Added {jobs_added} jobs from page {page + 1}\")\n",
    "                \n",
    "                # Save to database after each page\n",
    "                if jobs_added > 0 and self.db_connection:\n",
    "                    saved_count = self.save_to_database()\n",
    "                    print(f\"ðŸ’¾ Saved {saved_count} jobs to database\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()\n",
    "                print(\"âœ… Database connection closed\")\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _extract_page_jobs_css(self):\n",
    "        \"\"\"Extract jobs using reliable CSS selectors\"\"\"\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for card in job_cards:\n",
    "                job_info = self._extract_job_info_css(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_css(self, card):\n",
    "        \"\"\"Extract job information using CSS selectors\"\"\"\n",
    "        try:\n",
    "            # Title\n",
    "            title = self._safe_extract(card, 'h2.jobTitle', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            # Company\n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            \n",
    "            # Location\n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Salary\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_text = self._safe_extract(card, '[data-testid=\"attribute_snippet_testid\"]', 'text')\n",
    "            if salary_text and '$' in salary_text:\n",
    "                salary = salary_text\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type\n",
    "            job_type = self._extract_job_type(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'scraped_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        \"\"\"Safely extract element text or attribute\"\"\"\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _extract_job_type(self, card):\n",
    "        \"\"\"Extract job type\"\"\"\n",
    "        try:\n",
    "            card_text = card.text.lower()\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def save_to_database(self):\n",
    "        \"\"\"Save current jobs data to database\"\"\"\n",
    "        if not self.jobs_data or not self.db_connection:\n",
    "            print(\"No data to save or database not connected\")\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            saved_count = 0\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT OR IGNORE INTO jobs (title, company, location, salary, job_type, link, scraped_date)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            for job in self.jobs_data:\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (\n",
    "                        job['title'],\n",
    "                        job['company'],\n",
    "                        job['location'],\n",
    "                        job['salary'],\n",
    "                        job['job_type'],\n",
    "                        job['link'],\n",
    "                        job['scraped_date']\n",
    "                    ))\n",
    "                    if cursor.rowcount > 0:\n",
    "                        saved_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting job: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            self.db_connection.commit()\n",
    "            cursor.close()\n",
    "            return saved_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to database: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def query_jobs_from_database(self, limit=10):\n",
    "        \"\"\"Query jobs from database\"\"\"\n",
    "        if not self.db_connection:\n",
    "            print(\"Database not connected\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Reconnect if needed\n",
    "            if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "                return []\n",
    "                \n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT title, company, location, salary, job_type, link, scraped_date \n",
    "                FROM jobs \n",
    "                ORDER BY scraped_date DESC \n",
    "                LIMIT ?\n",
    "            \"\"\", (limit,))\n",
    "            \n",
    "            results = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            jobs = []\n",
    "            for row in results:\n",
    "                jobs.append({\n",
    "                    'title': row[0],\n",
    "                    'company': row[1],\n",
    "                    'location': row[2],\n",
    "                    'salary': row[3],\n",
    "                    'job_type': row[4],\n",
    "                    'link': row[5],\n",
    "                    'scraped_date': row[6]\n",
    "                })\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error querying database: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_database_stats(self):\n",
    "        \"\"\"Get statistics from the database\"\"\"\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Total jobs count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "            total_jobs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Companies count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT company) FROM jobs\")\n",
    "            unique_companies = cursor.fetchone()[0]\n",
    "            \n",
    "            # Locations count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT location) FROM jobs\")\n",
    "            unique_locations = cursor.fetchone()[0]\n",
    "            \n",
    "            # Latest scrape date\n",
    "            cursor.execute(\"SELECT MAX(scraped_date) FROM jobs\")\n",
    "            latest_scrape = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'total_jobs': total_jobs,\n",
    "                'unique_companies': unique_companies,\n",
    "                'unique_locations': unique_locations,\n",
    "                'latest_scrape': latest_scrape\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting database stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def save_to_csv(self, filename=\"indeed_jobs_backup.csv\"):\n",
    "        \"\"\"Backup database to CSV file\"\"\"\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM jobs\")\n",
    "            \n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(f\"ðŸ’¾ Database backed up to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error backing up to CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "    def export_to_excel(self, filename=\"indeed_jobs.xlsx\"):\n",
    "        \"\"\"Export database to Excel file\"\"\"\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            df = pd.read_sql_query(\"SELECT * FROM jobs\", conn)\n",
    "            df.to_excel(filename, index=False)\n",
    "            conn.close()\n",
    "            print(f\"ðŸ’¾ Database exported to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting to Excel: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ Indeed Scraper with SQLite Database\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = IndeedScraperWithDatabase(headless=True)\n",
    "    \n",
    "    # Setup SQLite database (no external dependencies needed)\n",
    "    if not scraper.setup_sqlite_database(\"indeed_jobs.sqlite\"):\n",
    "        print(\"Failed to setup database. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Get user input\n",
    "    job_title = input(\"Enter job title (default: developer): \").strip() or \"developer\"\n",
    "    location = input(\"Enter location (default: Canada): \").strip() or \"Canada\"\n",
    "    pages = input(\"Enter number of pages to scrape (default: 2): \").strip()\n",
    "    pages = int(pages) if pages.isdigit() else 2\n",
    "    \n",
    "    print(f\"\\nScraping '{job_title}' jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Scrape and save to database\n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nâœ… Successfully processed {len(jobs)} jobs in {end_time - start_time:.1f} seconds\")\n",
    "        \n",
    "        # Get database statistics\n",
    "        stats = scraper.get_database_stats()\n",
    "        print(f\"\\nðŸ“Š DATABASE STATISTICS:\")\n",
    "        print(f\"   â€¢ Total jobs in database: {stats.get('total_jobs', 0)}\")\n",
    "        print(f\"   â€¢ Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "        print(f\"   â€¢ Unique locations: {stats.get('unique_locations', 0)}\")\n",
    "        print(f\"   â€¢ Latest scrape: {stats.get('latest_scrape', 'N/A')}\")\n",
    "        \n",
    "        # Query and display recent jobs from database\n",
    "        recent_jobs = scraper.query_jobs_from_database(limit=5)\n",
    "        if recent_jobs:\n",
    "            print(f\"\\nðŸ“‹ RECENT JOBS FROM DATABASE:\")\n",
    "            for i, job in enumerate(recent_jobs, 1):\n",
    "                print(f\"\\n{i}. {job['title']}\")\n",
    "                print(f\"   Company: {job['company']}\")\n",
    "                print(f\"   Location: {job['location']}\")\n",
    "                print(f\"   Salary: {job['salary']}\")\n",
    "                print(f\"   Scraped: {job['scraped_date']}\")\n",
    "        \n",
    "        # Backup to CSV and Excel\n",
    "        scraper.save_to_csv(\"indeed_jobs_backup.csv\")\n",
    "        scraper.export_to_excel(\"indeed_jobs.xlsx\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No jobs were scraped\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Database file: indeed_jobs.sqlite\")\n",
    "    print(\"ðŸ’¾ CSV backup: indeed_jobs_backup.csv\")\n",
    "    print(\"ðŸ’¾ Excel export: indeed_jobs.xlsx\")\n",
    "    print(\"\\nðŸ”§ You can open the SQLite database with:\")\n",
    "    print(\"   - DB Browser for SQLite (GUI)\")\n",
    "    print(\"   - sqlite3 command line tool\")\n",
    "    print(\"   - Any SQL client that supports SQLite\")\n",
    "\n",
    "# Function to query the database later\n",
    "def query_database_later():\n",
    "    \"\"\"Function to query the database after scraping is complete\"\"\"\n",
    "    scraper = IndeedScraperWithDa\n",
    "    tabase()\n",
    "    jobs = scraper.query_jobs_from_database(limit=50)\n",
    "    stats = scraper.get_database_stats()\n",
    "    \n",
    "    print(f\"ðŸ“Š Current database stats:\")\n",
    "    print(f\"   Total jobs: {stats.get('total_jobs', 0)}\")\n",
    "    print(f\"   Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "    \n",
    "    return jobs, stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa735277-997f-4530-947d-7cea79fa1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database not connected\n",
      "ðŸ“Š Current database stats:\n",
      "   Total jobs: 33\n",
      "   Unique companies: 30\n"
     ]
    }
   ],
   "source": [
    "# Query the database anytime\n",
    "jobs, stats = query_database_later()\n",
    "\n",
    "# Or use DB Browser for SQLite (free GUI tool) to browse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b38ea2bc-9d8a-4c2b-8504-c1ac4edd9025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs table created/verified\n",
      "SQLite database connected: indeed_jobs.sqlite\n",
      "\n",
      "Scraping all jobs in 'Canada' (5 pages)...\n",
      "Scraping page 1...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 1\n",
      "ðŸ’¾ Saved 5 jobs to database\n",
      "Scraping page 2...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=10\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 2\n",
      "ðŸ’¾ Saved 6 jobs to database\n",
      "Scraping page 3...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=20\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 3\n",
      "ðŸ’¾ Saved 4 jobs to database\n",
      "Scraping page 4...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=30\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 4\n",
      "ðŸ’¾ Saved 3 jobs to database\n",
      "Scraping page 5...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=40\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 5\n",
      "ðŸ’¾ Saved 3 jobs to database\n",
      "Database connection closed\n",
      "\n",
      "Successfully processed 75 jobs in 51.5 seconds\n",
      "\n",
      "DATABASE STATISTICS:\n",
      "   â€¢ Total jobs in database: 90\n",
      "   â€¢ Unique companies: 78\n",
      "   â€¢ Unique locations: 79\n",
      "   â€¢ Latest scrape: 2025-10-22 21:00:11\n",
      "\n",
      "RECENT JOBS FROM DATABASE:\n",
      "\n",
      "1. Customer Service Agent - (Remote / Full-time / Overnight / No Calls)\n",
      "   Company: Seedbox\n",
      "   Location: Remote in MontrÃ©al, QC\n",
      "   Salary: $16.50 an hour\n",
      "   Scraped: 2025-10-22 21:00:11\n",
      "\n",
      "2. Virtual Customer Service Representative - Canada Proper\n",
      "   Company: IKS Health\n",
      "   Location: Remote\n",
      "   Salary: From $19 an hour\n",
      "   Scraped: 2025-10-22 21:00:11\n",
      "\n",
      "3. Cashier\n",
      "   Company: Pilot Company\n",
      "   Location: Balgonie, SK\n",
      "   Salary: $15.35â€“$17.60 an hour\n",
      "   Scraped: 2025-10-22 21:00:11\n",
      "\n",
      "4. Train Conductor\n",
      "   Company: CPKC\n",
      "   Location: Revelstoke, BC\n",
      "   Salary: $85,000â€“$100,000 a year\n",
      "   Scraped: 2025-10-22 21:00:01\n",
      "\n",
      "5. General Clerk\n",
      "   Company: Save-On-Foods\n",
      "   Location: Princeton, BC\n",
      "   Salary: $17.85â€“$21.00 an hour\n",
      "   Scraped: 2025-10-22 21:00:00\n",
      "Database backed up to indeed_jobs_backup.csv\n",
      "\n",
      "Database file: indeed_jobs.sqlite\n",
      "CSV backup: indeed_jobs_backup.csv\n",
      "Excel export: indeed_jobs.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class IndeedScraperWithDatabase:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "        self.db_connection = None\n",
    "        \n",
    "    def setup_sqlite_database(self, db_path=\"indeed_jobs.sqlite\"):\n",
    "        try:\n",
    "            self.db_connection = sqlite3.connect(db_path)\n",
    "            self._create_jobs_table()\n",
    "            print(f\"SQLite database connected: {db_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up SQLite database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_jobs_table(self):\n",
    "\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            company TEXT,\n",
    "            location TEXT,\n",
    "            salary TEXT,\n",
    "            job_type TEXT,\n",
    "            link TEXT,\n",
    "            scraped_date TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(title, company, location)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = self.db_connection.cursor()\n",
    "        cursor.execute(create_table_sql)\n",
    "        self.db_connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Jobs table created/verified\")\n",
    "\n",
    "    def scrape_jobs(self, job_title=\"\", location=\"Canada\", max_pages=5):\n",
    "        \n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"Scraping page {page + 1}...\")\n",
    "                print(f\"URL: {url}\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_css()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"Added {jobs_added} jobs from page {page + 1}\")\n",
    "                \n",
    "                # Save to database after each page\n",
    "                if jobs_added > 0 and self.db_connection:\n",
    "                    saved_count = self.save_to_database()\n",
    "                    print(f\"Saved {saved_count} jobs to database\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()\n",
    "                print(\"Database connection closed\")\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _extract_page_jobs_css(self):\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for card in job_cards:\n",
    "                job_info = self._extract_job_info_css(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_css(self, card):\n",
    "        try:\n",
    "            # Title\n",
    "            title = self._safe_extract(card, 'h2.jobTitle', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            # Company\n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            \n",
    "            # Location\n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Salary\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_text = self._safe_extract(card, '[data-testid=\"attribute_snippet_testid\"]', 'text')\n",
    "            if salary_text and '$' in salary_text:\n",
    "                salary = salary_text\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type\n",
    "            job_type = self._extract_job_type(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'scraped_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _extract_job_type(self, card):\n",
    "        try:\n",
    "            card_text = card.text.lower()\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def save_to_database(self):\n",
    "        if not self.jobs_data or not self.db_connection:\n",
    "            print(\"No data to save or database not connected\")\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            saved_count = 0\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT OR IGNORE INTO jobs (title, company, location, salary, job_type, link, scraped_date)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            for job in self.jobs_data:\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (\n",
    "                        job['title'],\n",
    "                        job['company'],\n",
    "                        job['location'],\n",
    "                        job['salary'],\n",
    "                        job['job_type'],\n",
    "                        job['link'],\n",
    "                        job['scraped_date']\n",
    "                    ))\n",
    "                    if cursor.rowcount > 0:\n",
    "                        saved_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting job: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            self.db_connection.commit()\n",
    "            cursor.close()\n",
    "            return saved_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to database: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def query_jobs_from_database(self, limit=10):\n",
    "        if not self.db_connection:\n",
    "            print(\"Database not connected\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Reconnect if needed\n",
    "            if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "                return []\n",
    "                \n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT title, company, location, salary, job_type, link, scraped_date \n",
    "                FROM jobs \n",
    "                ORDER BY scraped_date DESC \n",
    "                LIMIT ?\n",
    "            \"\"\", (limit,))\n",
    "            \n",
    "            results = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            jobs = []\n",
    "            for row in results:\n",
    "                jobs.append({\n",
    "                    'title': row[0],\n",
    "                    'company': row[1],\n",
    "                    'location': row[2],\n",
    "                    'salary': row[3],\n",
    "                    'job_type': row[4],\n",
    "                    'link': row[5],\n",
    "                    'scraped_date': row[6]\n",
    "                })\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error querying database: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_database_stats(self):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Total jobs count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "            total_jobs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Companies count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT company) FROM jobs\")\n",
    "            unique_companies = cursor.fetchone()[0]\n",
    "            \n",
    "            # Locations count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT location) FROM jobs\")\n",
    "            unique_locations = cursor.fetchone()[0]\n",
    "            \n",
    "            # Latest scrape date\n",
    "            cursor.execute(\"SELECT MAX(scraped_date) FROM jobs\")\n",
    "            latest_scrape = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'total_jobs': total_jobs,\n",
    "                'unique_companies': unique_companies,\n",
    "                'unique_locations': unique_locations,\n",
    "                'latest_scrape': latest_scrape\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting database stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def save_to_csv(self, filename=\"indeed_jobs_backup.csv\"):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM jobs\")\n",
    "            \n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(f\"Database backed up to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error backing up to CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "    def export_to_excel(self, filename=\"indeed_jobs.xlsx\"):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            df = pd.read_sql_query(\"SELECT * FROM jobs\", conn)\n",
    "            df.to_excel(filename, index=False)\n",
    "            conn.close()\n",
    "            print(f\"Database exported to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting to Excel: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = IndeedScraperWithDatabase(headless=True)\n",
    "    \n",
    "    # Setup SQLite database\n",
    "    if not scraper.setup_sqlite_database(\"indeed_jobs.sqlite\"):\n",
    "        print(\"Failed to setup database. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    job_title = \"\"  \n",
    "    location = \"Canada\"\n",
    "    pages = 5\n",
    "    \n",
    "    print(f\"\\nScraping all jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Scrape and save to database\n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nSuccessfully processed {len(jobs)} jobs in {end_time - start_time:.1f} seconds\")\n",
    "        \n",
    "        # Get database statistics\n",
    "        stats = scraper.get_database_stats()\n",
    "        print(f\"\\nDATABASE STATISTICS:\")\n",
    "        print(f\"   â€¢ Total jobs in database: {stats.get('total_jobs', 0)}\")\n",
    "        print(f\"   â€¢ Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "        print(f\"   â€¢ Unique locations: {stats.get('unique_locations', 0)}\")\n",
    "        print(f\"   â€¢ Latest scrape: {stats.get('latest_scrape', 'N/A')}\")\n",
    "        \n",
    "        # Query and display recent jobs from database\n",
    "        recent_jobs = scraper.query_jobs_from_database(limit=5)\n",
    "        if recent_jobs:\n",
    "            print(f\"\\nRECENT JOBS FROM DATABASE:\")\n",
    "            for i, job in enumerate(recent_jobs, 1):\n",
    "                print(f\"\\n{i}. {job['title']}\")\n",
    "                print(f\"   Company: {job['company']}\")\n",
    "                print(f\"   Location: {job['location']}\")\n",
    "                print(f\"   Salary: {job['salary']}\")\n",
    "                print(f\"   Scraped: {job['scraped_date']}\")\n",
    "        \n",
    "        # Backup to CSV\n",
    "        scraper.save_to_csv(\"indeed_jobs_backup.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No jobs were scraped\")\n",
    "    \n",
    "    print(f\"\\nDatabase file: indeed_jobs.sqlite\")\n",
    "    print(\"CSV backup: indeed_jobs_backup.csv\")\n",
    "\n",
    "def query_database_later():\n",
    "    scraper = IndeedScraperWithDatabase()\n",
    "    jobs = scraper.query_jobs_from_database(limit=50)\n",
    "    stats = scraper.get_database_stats()\n",
    "    \n",
    "    print(f\"Current database stats:\")\n",
    "    print(f\"   Total jobs: {stats.get('total_jobs', 0)}\")\n",
    "    print(f\"   Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "    \n",
    "    return jobs, stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19582e05-3eb7-4c83-a252-91c65c93cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database not connected\n",
      "Current database stats:\n",
      "   Total jobs: 90\n",
      "   Unique companies: 78\n"
     ]
    }
   ],
   "source": [
    "# Query the database anytime\n",
    "jobs, stats = query_database_later()\n",
    "\n",
    "# Or use DB Browser for SQLite (free GUI tool) to browse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb27997-f0d0-4030-978f-79147ad23ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs table created/verified\n",
      "SQLite database connected: indeed_jobs.sqlite\n",
      "\n",
      "Scraping all jobs in 'Canada' (5 pages)...\n",
      "Scraping page 1...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 1\n",
      "Saved 14 jobs to database\n",
      "Scraping page 2...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=10\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 2\n",
      "Saved 15 jobs to database\n",
      "Scraping page 3...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=20\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 3\n",
      "Saved 12 jobs to database\n",
      "Scraping page 4...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=30\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 4\n",
      "Saved 13 jobs to database\n",
      "Scraping page 5...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=40\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 5\n",
      "Saved 10 jobs to database\n",
      "Database connection closed\n",
      "\n",
      "Successfully processed 75 jobs in 88.4 seconds\n",
      "\n",
      "DATABASE STATISTICS:\n",
      "   â€¢ Total jobs in database: 64\n",
      "   â€¢ Unique companies: 62\n",
      "   â€¢ Unique locations: 52\n",
      "   â€¢ Jobs with descriptions: 9\n",
      "   â€¢ Latest scrape: 2025-10-22 21:30:03\n",
      "\n",
      "RECENT JOBS FROM DATABASE:\n",
      "\n",
      "1. Dishwasher\n",
      "   Company: Freds - Kitsilano\n",
      "   Location: Vancouver, BC V6J 1M4\n",
      "   Salary: $17.85 an hour\n",
      "   Type: Not specified\n",
      "   Description: Description snippet not available...\n",
      "   Scraped: 2025-10-22 21:30:03\n",
      "\n",
      "2. greenhouse labourer\n",
      "   Company: Bloomhouse Cannabis Co\n",
      "   Location: Lively, ON P3Y 1L2\n",
      "   Salary: $18 an hour\n",
      "   Type: Not specified\n",
      "   Description: Description snippet not available...\n",
      "   Scraped: 2025-10-22 21:30:01\n",
      "\n",
      "3. SANAJIKSANUT - CORESHACK HELPER (FLY-IN FLY-OUT)\n",
      "   Company: Agnico-Eagle Mines Limited\n",
      "   Location: Rankin Inlet, NU\n",
      "   Salary: Salary not specified\n",
      "   Type: Not specified\n",
      "   Description: Description snippet not available...\n",
      "   Scraped: 2025-10-22 21:30:01\n",
      "Database backed up to indeed_jobs_backup.csv\n",
      "\n",
      "Database file: indeed_jobs.sqlite\n",
      "CSV backup: indeed_jobs_backup.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class IndeedScraperWithDatabase:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "        self.db_connection = None\n",
    "        \n",
    "    def setup_sqlite_database(self, db_path=\"indeed_jobs.sqlite\"):\n",
    "        try:\n",
    "            self.db_connection = sqlite3.connect(db_path)\n",
    "            self._create_jobs_table()\n",
    "            print(f\"SQLite database connected: {db_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up SQLite database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_jobs_table(self):\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            company TEXT,\n",
    "            location TEXT,\n",
    "            salary TEXT,\n",
    "            job_type TEXT,\n",
    "            link TEXT,\n",
    "            description TEXT,\n",
    "            scraped_date TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(title, company, location)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = self.db_connection.cursor()\n",
    "        cursor.execute(create_table_sql)\n",
    "        self.db_connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Jobs table created/verified\")\n",
    "\n",
    "    def scrape_jobs(self, job_title=\"\", location=\"Canada\", max_pages=5):\n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"Scraping page {page + 1}...\")\n",
    "                print(f\"URL: {url}\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_css()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"Added {jobs_added} jobs from page {page + 1}\")\n",
    "                \n",
    "                # Save to database after each page\n",
    "                if jobs_added > 0 and self.db_connection:\n",
    "                    saved_count = self.save_to_database()\n",
    "                    print(f\"Saved {saved_count} jobs to database\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()\n",
    "                print(\"Database connection closed\")\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _extract_page_jobs_css(self):\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for i, card in enumerate(job_cards):\n",
    "                # Add delay to avoid being blocked\n",
    "                if i > 0 and i % 3 == 0:\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                job_info = self._extract_job_info_css(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_css(self, card):\n",
    "        try:\n",
    "            # Title\n",
    "            title = self._safe_extract(card, 'h2.jobTitle', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            # Company\n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            \n",
    "            # Location\n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Salary\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_text = self._safe_extract(card, '[data-testid=\"attribute_snippet_testid\"]', 'text')\n",
    "            if salary_text and '$' in salary_text:\n",
    "                salary = salary_text\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type\n",
    "            job_type = self._extract_job_type(card)\n",
    "            \n",
    "            # Job Description - Extract from snippet on main page\n",
    "            description = self._extract_job_description(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'description': description or \"Description not available\",\n",
    "                    'scraped_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _extract_job_description(self, card):\n",
    "        \"\"\"Extract job description from the job card snippet\"\"\"\n",
    "        try:\n",
    "            # Try multiple selectors for job description snippets\n",
    "            description_selectors = [\n",
    "                'div[class*=\"job-snippet\"]',\n",
    "                'div[class*=\"description-snippet\"]',\n",
    "                'div[class*=\"summary\"]',\n",
    "                '.job-snippet',\n",
    "                '.job-snippet ul',\n",
    "                'div[class*=\"metadata\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in description_selectors:\n",
    "                description = self._safe_extract(card, selector, 'text')\n",
    "                if description and len(description.strip()) > 20:  # Minimum length to be meaningful\n",
    "                    # Clean up the description\n",
    "                    description = description.strip()\n",
    "                    # Remove extra whitespace and newlines\n",
    "                    description = re.sub(r'\\s+', ' ', description)\n",
    "                    return description[:1000]  # Limit to 1000 characters\n",
    "            \n",
    "            # If no specific description element found, try to get key details from the card\n",
    "            card_text = card.text\n",
    "            lines = [line.strip() for line in card_text.split('\\n') if line.strip()]\n",
    "            \n",
    "            # Look for description-like text (usually appears after company and location)\n",
    "            description_keywords = ['responsibilities', 'requirements', 'qualifications', 'experience', 'skills', 'duties']\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                line_lower = line.lower()\n",
    "                # Skip if it's title, company, location, salary, or job type\n",
    "                if (any(keyword in line_lower for keyword in description_keywords) or\n",
    "                    (len(line) > 50 and not any(indicator in line_lower for indicator in \n",
    "                     ['$', 'salary', 'full-time', 'part-time', 'contract', 'remote', 'temporary']))):\n",
    "                    return line[:500]  # Return first 500 characters\n",
    "            \n",
    "            return \"Description snippet not available\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting description: {e}\")\n",
    "            return \"Error extracting description\"\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _extract_job_type(self, card):\n",
    "        try:\n",
    "            card_text = card.text.lower()\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def save_to_database(self):\n",
    "        if not self.jobs_data or not self.db_connection:\n",
    "            print(\"No data to save or database not connected\")\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            saved_count = 0\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT OR IGNORE INTO jobs (title, company, location, salary, job_type, link, description, scraped_date)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            for job in self.jobs_data:\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (\n",
    "                        job['title'],\n",
    "                        job['company'],\n",
    "                        job['location'],\n",
    "                        job['salary'],\n",
    "                        job['job_type'],\n",
    "                        job['link'],\n",
    "                        job['description'],\n",
    "                        job['scraped_date']\n",
    "                    ))\n",
    "                    if cursor.rowcount > 0:\n",
    "                        saved_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting job: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            self.db_connection.commit()\n",
    "            cursor.close()\n",
    "            return saved_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to database: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def query_jobs_from_database(self, limit=10):\n",
    "        if not self.db_connection:\n",
    "            print(\"Database not connected\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Reconnect if needed\n",
    "            if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "                return []\n",
    "                \n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT title, company, location, salary, job_type, link, description, scraped_date \n",
    "                FROM jobs \n",
    "                ORDER BY scraped_date DESC \n",
    "                LIMIT ?\n",
    "            \"\"\", (limit,))\n",
    "            \n",
    "            results = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            jobs = []\n",
    "            for row in results:\n",
    "                jobs.append({\n",
    "                    'title': row[0],\n",
    "                    'company': row[1],\n",
    "                    'location': row[2],\n",
    "                    'salary': row[3],\n",
    "                    'job_type': row[4],\n",
    "                    'link': row[5],\n",
    "                    'description': row[6],\n",
    "                    'scraped_date': row[7]\n",
    "                })\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error querying database: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_database_stats(self):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Total jobs count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "            total_jobs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Companies count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT company) FROM jobs\")\n",
    "            unique_companies = cursor.fetchone()[0]\n",
    "            \n",
    "            # Locations count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT location) FROM jobs\")\n",
    "            unique_locations = cursor.fetchone()[0]\n",
    "            \n",
    "            # Jobs with descriptions\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs WHERE description != 'Description snippet not available'\")\n",
    "            jobs_with_descriptions = cursor.fetchone()[0]\n",
    "            \n",
    "            # Latest scrape date\n",
    "            cursor.execute(\"SELECT MAX(scraped_date) FROM jobs\")\n",
    "            latest_scrape = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'total_jobs': total_jobs,\n",
    "                'unique_companies': unique_companies,\n",
    "                'unique_locations': unique_locations,\n",
    "                'jobs_with_descriptions': jobs_with_descriptions,\n",
    "                'latest_scrape': latest_scrape\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting database stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def save_to_csv(self, filename=\"indeed_jobs_backup.csv\"):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM jobs\")\n",
    "            \n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(f\"Database backed up to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error backing up to CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "    def export_to_excel(self, filename=\"indeed_jobs.xlsx\"):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            df = pd.read_sql_query(\"SELECT * FROM jobs\", conn)\n",
    "            df.to_excel(filename, index=False)\n",
    "            conn.close()\n",
    "            print(f\"Database exported to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting to Excel: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    # Initialize scraper\n",
    "    scraper = IndeedScraperWithDatabase(headless=True)\n",
    "    \n",
    "    # Setup SQLite database\n",
    "    if not scraper.setup_sqlite_database(\"indeed_jobs.sqlite\"):\n",
    "        print(\"Failed to setup database. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    job_title = \"\"  \n",
    "    location = \"Canada\"\n",
    "    pages = 5\n",
    "    \n",
    "    print(f\"\\nScraping all jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Scrape and save to database\n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nSuccessfully processed {len(jobs)} jobs in {end_time - start_time:.1f} seconds\")\n",
    "        \n",
    "        # Get database statistics\n",
    "        stats = scraper.get_database_stats()\n",
    "        print(f\"\\nDATABASE STATISTICS:\")\n",
    "        print(f\"   â€¢ Total jobs in database: {stats.get('total_jobs', 0)}\")\n",
    "        print(f\"   â€¢ Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "        print(f\"   â€¢ Unique locations: {stats.get('unique_locations', 0)}\")\n",
    "        print(f\"   â€¢ Jobs with descriptions: {stats.get('jobs_with_descriptions', 0)}\")\n",
    "        print(f\"   â€¢ Latest scrape: {stats.get('latest_scrape', 'N/A')}\")\n",
    "        \n",
    "        # Query and display recent jobs from database\n",
    "        recent_jobs = scraper.query_jobs_from_database(limit=3)\n",
    "        if recent_jobs:\n",
    "            print(f\"\\nRECENT JOBS FROM DATABASE:\")\n",
    "            for i, job in enumerate(recent_jobs, 1):\n",
    "                print(f\"\\n{i}. {job['title']}\")\n",
    "                print(f\"   Company: {job['company']}\")\n",
    "                print(f\"   Location: {job['location']}\")\n",
    "                print(f\"   Salary: {job['salary']}\")\n",
    "                print(f\"   Type: {job['job_type']}\")\n",
    "                print(f\"   Description: {job['description'][:150]}...\")\n",
    "                print(f\"   Scraped: {job['scraped_date']}\")\n",
    "        \n",
    "        # Backup to CSV\n",
    "        scraper.save_to_csv(\"indeed_jobs_backup.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No jobs were scraped\")\n",
    "    \n",
    "    print(f\"\\nDatabase file: indeed_jobs.sqlite\")\n",
    "    print(\"CSV backup: indeed_jobs_backup.csv\")\n",
    "\n",
    "def query_database_later():\n",
    "    scraper = IndeedScraperWithDatabase()\n",
    "    jobs = scraper.query_jobs_from_database(limit=50)\n",
    "    stats = scraper.get_database_stats()\n",
    "    \n",
    "    print(f\"Current database stats:\")\n",
    "    print(f\"   Total jobs: {stats.get('total_jobs', 0)}\")\n",
    "    print(f\"   Unique companies: {stats.get('unique_companies', 0)}\")\n",
    "    print(f\"   Jobs with descriptions: {stats.get('jobs_with_descriptions', 0)}\")\n",
    "    \n",
    "    return jobs, stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae24594-9f9b-46fc-bc31-71626ae200d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs table created/verified\n",
      "SQLite database connected: indeed_jobs.sqlite\n",
      "\n",
      "Scraping all jobs in 'Canada' (3 pages)...\n",
      "Scraping page 1...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 1\n",
      "Saved 4 jobs to database\n",
      "Scraping page 2...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=10\n",
      "No job cards found on page 2\n",
      "Scraping page 3...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=20\n",
      "Found 16 job cards\n",
      "Added 15 jobs from page 3\n",
      "Saved 5 jobs to database\n",
      "Database connection closed\n",
      "\n",
      "Successfully processed 30 jobs in 76.5 seconds\n",
      "\n",
      "SAMPLE JOBS SCRAPED:\n",
      "\n",
      "1. Invetory Counter\n",
      "   Company: Bannons Gas Bar\n",
      "   Location: Thunder Bay, ON P7J 1K5\n",
      "   Salary: $17.20â€“$20.00 an hour\n",
      "   Type: Full-time\n",
      "   Description: Description snippet not available...\n",
      "\n",
      "2. Housekeeper/Cleaner\n",
      "   Company: Genie Clean\n",
      "   Location: Williams Lake, BC\n",
      "   Salary: $21.31â€“$22.03 an hour\n",
      "   Type: Full-time\n",
      "   Description: Description snippet not available...\n",
      "\n",
      "3. Office Assistant/Receptionist\n",
      "   Company: Precision Services\n",
      "   Location: Massey, ON\n",
      "   Salary: $18â€“$23 an hour\n",
      "   Type: Part-time\n",
      "   Description: Description snippet not available...\n",
      "\n",
      "DATABASE STATISTICS:\n",
      "   â€¢ Total jobs in database: 73\n",
      "   â€¢ Unique companies: 70\n",
      "   â€¢ Jobs with descriptions: 11\n",
      "Database backed up to indeed_jobs_backup.csv\n",
      "\n",
      "Database file: indeed_jobs.sqlite\n",
      "CSV backup: indeed_jobs_backup.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ImprovedIndeedScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        self.options.add_argument('--disable-gpu')\n",
    "        self.options.add_argument('--disable-extensions')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "        self.db_connection = None\n",
    "        \n",
    "    def setup_sqlite_database(self, db_path=\"indeed_jobs.sqlite\"):\n",
    "        try:\n",
    "            self.db_connection = sqlite3.connect(db_path)\n",
    "            self._create_jobs_table()\n",
    "            print(f\"SQLite database connected: {db_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up SQLite database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_jobs_table(self):\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            company TEXT,\n",
    "            location TEXT,\n",
    "            salary TEXT,\n",
    "            job_type TEXT,\n",
    "            link TEXT,\n",
    "            description TEXT,\n",
    "            scraped_date TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(title, company, location)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = self.db_connection.cursor()\n",
    "        cursor.execute(create_table_sql)\n",
    "        self.db_connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Jobs table created/verified\")\n",
    "\n",
    "    def scrape_jobs(self, job_title=\"\", location=\"Canada\", max_pages=5):\n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"Scraping page {page + 1}...\")\n",
    "                print(f\"URL: {url}\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                # Handle cookie consent if present\n",
    "                self._handle_cookie_consent()\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_improved()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"Added {jobs_added} jobs from page {page + 1}\")\n",
    "                \n",
    "                # Save to database after each page\n",
    "                if jobs_added > 0 and self.db_connection:\n",
    "                    saved_count = self.save_to_database()\n",
    "                    print(f\"Saved {saved_count} jobs to database\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()\n",
    "                print(\"Database connection closed\")\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _handle_cookie_consent(self):\n",
    "        \"\"\"Handle cookie consent popup if it appears\"\"\"\n",
    "        try:\n",
    "            cookie_selectors = [\n",
    "                'button[aria-label=\"reject\"]',\n",
    "                'button[aria-label=\"Reject All\"]',\n",
    "                'button#onetrust-reject-all-handler',\n",
    "                'button[data-testid=\"reject-button\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in cookie_selectors:\n",
    "                try:\n",
    "                    reject_button = WebDriverWait(self.driver, 2).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "                    )\n",
    "                    reject_button.click()\n",
    "                    print(\"Cookie consent handled\")\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def _extract_page_jobs_improved(self):\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for i, card in enumerate(job_cards):\n",
    "                # Add delay to avoid being blocked\n",
    "                if i > 0 and i % 3 == 0:\n",
    "                    time.sleep(random.uniform(1, 2))\n",
    "                \n",
    "                job_info = self._extract_job_info_improved(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_improved(self, card):\n",
    "        try:\n",
    "            # Extract basic info first\n",
    "            title = self._safe_extract(card, 'h2.jobTitle a', 'text')\n",
    "            if not title:\n",
    "                title = self._safe_extract(card, 'h2 a', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '.companyName', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            \n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '.companyLocation', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Now extract description using the basic info we have\n",
    "            description = self._extract_job_description_improved(card, title, company, location)\n",
    "            \n",
    "            # Salary - Improved extraction\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_selectors = [\n",
    "                '[data-testid=\"attribute_snippet_testid\"]',\n",
    "                '.salary-snippet-container',\n",
    "                '.metadata salary-snippet-container'\n",
    "            ]\n",
    "            \n",
    "            for selector in salary_selectors:\n",
    "                salary_text = self._safe_extract(card, selector, 'text')\n",
    "                if salary_text and ('$' in salary_text or 'hour' in salary_text.lower() or 'year' in salary_text.lower()):\n",
    "                    salary = salary_text\n",
    "                    break\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if not link:\n",
    "                link = self._safe_extract(card, 'h2 a', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type - Improved extraction\n",
    "            job_type = self._extract_job_type_improved(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'description': description or \"Description not available\",\n",
    "                    'scraped_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _extract_job_description_improved(self, card, title, company, location):\n",
    "        \"\"\"Improved job description extraction with proper parameters\"\"\"\n",
    "        try:\n",
    "            # Try the job snippet/summary section first\n",
    "            description_selectors = [\n",
    "                'div.job-snippet',\n",
    "                'div[class*=\"job-snippet\"]',\n",
    "                'div[class*=\"snippet\"]',\n",
    "                'div[class*=\"summary\"]',\n",
    "                'ul[style*=\"list-style-type:circle\"]',\n",
    "                '.css-e9ucx3',  # From the HTML structure\n",
    "                '[data-testid=\"belowJobSnippet\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in description_selectors:\n",
    "                description = self._safe_extract(card, selector, 'text')\n",
    "                if description and len(description.strip()) > 20:\n",
    "                    description = description.strip()\n",
    "                    description = re.sub(r'\\s+', ' ', description)\n",
    "                    return description[:1000]  # Limit to 1000 characters\n",
    "            \n",
    "            # Try to extract from the entire card text with better filtering\n",
    "            card_text = card.text\n",
    "            if card_text:\n",
    "                lines = [line.strip() for line in card_text.split('\\n') if line.strip()]\n",
    "                \n",
    "                # Look for description content after basic info\n",
    "                basic_info_indicators = ['$', 'salary', 'full-time', 'part-time', 'contract', \n",
    "                                       'remote', 'temporary', 'permanent', 'urgently hiring',\n",
    "                                       'apply', 'save', 'bookmark', 'easily apply']\n",
    "                \n",
    "                description_lines = []\n",
    "                \n",
    "                for line in lines:\n",
    "                    line_lower = line.lower()\n",
    "                    \n",
    "                    # Skip if it's basic info we've already captured\n",
    "                    if any(indicator in line_lower for indicator in basic_info_indicators):\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip if it's title, company, or location (usually shorter lines that match exactly)\n",
    "                    if (title and title.lower() in line_lower) or \\\n",
    "                       (company and company.lower() in line_lower) or \\\n",
    "                       (location and location.lower() in line_lower):\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip very short lines (likely navigation or buttons)\n",
    "                    if len(line) < 20:\n",
    "                        continue\n",
    "                        \n",
    "                    # If we find a reasonably long line that's not basic info, consider it description\n",
    "                    if len(line) > 30:\n",
    "                        description_lines.append(line)\n",
    "                \n",
    "                if description_lines:\n",
    "                    description = ' '.join(description_lines[:3])  # Take first 3 description lines\n",
    "                    description = re.sub(r'\\s+', ' ', description).strip()\n",
    "                    if len(description) > 30:\n",
    "                        return description[:800]\n",
    "            \n",
    "            return \"Description snippet not available\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting description: {e}\")\n",
    "            return \"Error extracting description\"\n",
    "\n",
    "    def _extract_job_type_improved(self, card):\n",
    "        \"\"\"Improved job type extraction\"\"\"\n",
    "        try:\n",
    "            card_text = card.text.lower()\n",
    "            \n",
    "            # Check for job type in metadata\n",
    "            metadata_selectors = [\n",
    "                '[data-testid=\"attribute_snippet_testid\"]',\n",
    "                '.metadata',\n",
    "                '.css-5ooe72'\n",
    "            ]\n",
    "            \n",
    "            for selector in metadata_selectors:\n",
    "                try:\n",
    "                    elements = card.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        text = element.text.lower()\n",
    "                        if 'full-time' in text:\n",
    "                            return 'Full-time'\n",
    "                        elif 'part-time' in text:\n",
    "                            return 'Part-time'\n",
    "                        elif 'contract' in text:\n",
    "                            return 'Contract'\n",
    "                        elif 'temporary' in text:\n",
    "                            return 'Temporary'\n",
    "                        elif 'permanent' in text:\n",
    "                            return 'Permanent'\n",
    "                        elif 'remote' in text:\n",
    "                            return 'Remote'\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Fallback to text analysis\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "                \n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def save_to_database(self):\n",
    "        if not self.jobs_data or not self.db_connection:\n",
    "            print(\"No data to save or database not connected\")\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            saved_count = 0\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT OR IGNORE INTO jobs (title, company, location, salary, job_type, link, description, scraped_date)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            for job in self.jobs_data:\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (\n",
    "                        job['title'],\n",
    "                        job['company'],\n",
    "                        job['location'],\n",
    "                        job['salary'],\n",
    "                        job['job_type'],\n",
    "                        job['link'],\n",
    "                        job['description'],\n",
    "                        job['scraped_date']\n",
    "                    ))\n",
    "                    if cursor.rowcount > 0:\n",
    "                        saved_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting job: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            self.db_connection.commit()\n",
    "            cursor.close()\n",
    "            return saved_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to database: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def query_jobs_from_database(self, limit=10):\n",
    "        if not self.db_connection:\n",
    "            print(\"Database not connected\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Reconnect if needed\n",
    "            if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "                return []\n",
    "                \n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT title, company, location, salary, job_type, link, description, scraped_date \n",
    "                FROM jobs \n",
    "                ORDER BY scraped_date DESC \n",
    "                LIMIT ?\n",
    "            \"\"\", (limit,))\n",
    "            \n",
    "            results = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            jobs = []\n",
    "            for row in results:\n",
    "                jobs.append({\n",
    "                    'title': row[0],\n",
    "                    'company': row[1],\n",
    "                    'location': row[2],\n",
    "                    'salary': row[3],\n",
    "                    'job_type': row[4],\n",
    "                    'link': row[5],\n",
    "                    'description': row[6],\n",
    "                    'scraped_date': row[7]\n",
    "                })\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error querying database: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_database_stats(self):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Total jobs count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "            total_jobs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Companies count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT company) FROM jobs\")\n",
    "            unique_companies = cursor.fetchone()[0]\n",
    "            \n",
    "            # Locations count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT location) FROM jobs\")\n",
    "            unique_locations = cursor.fetchone()[0]\n",
    "            \n",
    "            # Jobs with descriptions\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs WHERE description != 'Description snippet not available' AND description != 'Error extracting description'\")\n",
    "            jobs_with_descriptions = cursor.fetchone()[0]\n",
    "            \n",
    "            # Latest scrape date\n",
    "            cursor.execute(\"SELECT MAX(scraped_date) FROM jobs\")\n",
    "            latest_scrape = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'total_jobs': total_jobs,\n",
    "                'unique_companies': unique_companies,\n",
    "                'unique_locations': unique_locations,\n",
    "                'jobs_with_descriptions': jobs_with_descriptions,\n",
    "                'latest_scrape': latest_scrape\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting database stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def save_to_csv(self, filename=\"indeed_jobs_backup.csv\"):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM jobs\")\n",
    "            \n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(f\"Database backed up to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error backing up to CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "    def export_to_excel(self, filename=\"indeed_jobs.xlsx\"):\n",
    "        if not os.path.exists(\"indeed_jobs.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"indeed_jobs.sqlite\")\n",
    "            df = pd.read_sql_query(\"SELECT * FROM jobs\", conn)\n",
    "            df.to_excel(filename, index=False)\n",
    "            conn.close()\n",
    "            print(f\"Database exported to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting to Excel: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    # Initialize scraper\n",
    "    scraper = ImprovedIndeedScraper(headless=True)\n",
    "    \n",
    "    # Setup SQLite database\n",
    "    if not scraper.setup_sqlite_database(\"indeed_jobs.sqlite\"):\n",
    "        print(\"Failed to setup database. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    job_title = \"\"  \n",
    "    location = \"Canada\"\n",
    "    pages = 3  # Start with fewer pages for testing\n",
    "    \n",
    "    print(f\"\\nScraping all jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Scrape and save to database\n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nSuccessfully processed {len(jobs)} jobs in {end_time - start_time:.1f} seconds\")\n",
    "\n",
    "        # Backup to CSV\n",
    "        scraper.save_to_csv(\"indeed_jobs_backup.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No jobs were scraped\")\n",
    "    \n",
    "    print(f\"\\nDatabase file: indeed_jobs.sqlite\")\n",
    "    print(\"CSV backup: indeed_jobs_backup.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b8197-8970-41d0-91c8-71a42d2fdd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
