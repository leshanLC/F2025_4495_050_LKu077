{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ecb2387-935c-4e88-9112-f6658dd67077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs table created/verified\n",
      "SQLite database connected: pathfinder_db.sqlite\n",
      "\n",
      "Scraping all jobs in 'Canada' (3 pages)...\n",
      "Scraping page 1...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada\n",
      "Found 16 job cards\n",
      "Added jobs from page 1\n",
      "Saved jobs to database\n",
      "Scraping page 2...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=10\n",
      "Found 16 job cards\n",
      "Added jobs from page 2\n",
      "Saved jobs to database\n",
      "Scraping page 3...\n",
      "URL: https://ca.indeed.com/jobs?q=&l=Canada&start=20\n",
      "Found 16 job cards\n",
      "Added jobs from page 3\n",
      "Saved jobs to database\n",
      "Database connection closed\n",
      "\n",
      "Successfully processed jobs in 89.0 seconds\n",
      "Database backed up to indeed_jobs_backup.csv\n",
      "\n",
      "Database file: pathfinder_db.sqlite\n",
      "CSV backup: indeed_jobs_backup.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ImprovedIndeedScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        self.options = Options()\n",
    "        \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        self.options.add_argument('--disable-gpu')\n",
    "        self.options.add_argument('--disable-extensions')\n",
    "        \n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        if headless:\n",
    "            self.options.add_argument('--headless')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=self.options\n",
    "        )\n",
    "        \n",
    "        self.jobs_data = []\n",
    "        self.db_connection = None\n",
    "        \n",
    "    def setup_sqlite_database(self, db_path=\"pathfinder_db.sqlite\"):\n",
    "        try:\n",
    "            self.db_connection = sqlite3.connect(db_path)\n",
    "            self._create_jobs_table()\n",
    "            print(f\"SQLite database connected: {db_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up SQLite database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_jobs_table(self):\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            company TEXT,\n",
    "            location TEXT,\n",
    "            salary TEXT,\n",
    "            job_type TEXT,\n",
    "            link TEXT,\n",
    "            description TEXT,\n",
    "            scraped_date TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(title, company, location)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = self.db_connection.cursor()\n",
    "        cursor.execute(create_table_sql)\n",
    "        self.db_connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Jobs table created/verified\")\n",
    "\n",
    "    def scrape_jobs(self, job_title=\"\", location=\"Canada\", max_pages=5):\n",
    "        try:\n",
    "            for page in range(max_pages):\n",
    "                if page == 0:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}\"\n",
    "                else:\n",
    "                    url = f\"https://ca.indeed.com/jobs?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+').replace(',', '%2C')}&start={page * 10}\"\n",
    "                \n",
    "                print(f\"Scraping page {page + 1}...\")\n",
    "                print(f\"URL: {url}\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Wait for job cards\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.job_seen_beacon'))\n",
    "                    )\n",
    "                except:\n",
    "                    print(f\"No job cards found on page {page + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                # Handle cookie consent if present\n",
    "                self._handle_cookie_consent()\n",
    "                \n",
    "                jobs_count_before = len(self.jobs_data)\n",
    "                self._extract_page_jobs_improved()\n",
    "                jobs_added = len(self.jobs_data) - jobs_count_before\n",
    "                \n",
    "                print(f\"Added jobs from page {page + 1}\")\n",
    "                \n",
    "                # Save to database after each page\n",
    "                if jobs_added > 0 and self.db_connection:\n",
    "                    saved_count = self.save_to_database()\n",
    "                    print(f\"Saved jobs to database\")\n",
    "                \n",
    "                if page < max_pages - 1:\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()\n",
    "                print(\"Database connection closed\")\n",
    "        \n",
    "        return self.jobs_data\n",
    "\n",
    "    def _handle_cookie_consent(self):\n",
    "        \"\"\"Handle cookie consent popup if it appears\"\"\"\n",
    "        try:\n",
    "            cookie_selectors = [\n",
    "                'button[aria-label=\"reject\"]',\n",
    "                'button[aria-label=\"Reject All\"]',\n",
    "                'button#onetrust-reject-all-handler',\n",
    "                'button[data-testid=\"reject-button\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in cookie_selectors:\n",
    "                try:\n",
    "                    reject_button = WebDriverWait(self.driver, 2).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "                    )\n",
    "                    reject_button.click()\n",
    "                    print(\"Cookie consent handled\")\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def _extract_page_jobs_improved(self):\n",
    "        try:\n",
    "            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')\n",
    "            print(f\"Found {len(job_cards)} job cards\")\n",
    "            \n",
    "            for i, card in enumerate(job_cards):\n",
    "                # Add delay to avoid being blocked\n",
    "                if i > 0 and i % 3 == 0:\n",
    "                    time.sleep(random.uniform(1, 2))\n",
    "                \n",
    "                job_info = self._extract_job_info_improved(card)\n",
    "                if job_info:\n",
    "                    self.jobs_data.append(job_info)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting jobs: {e}\")\n",
    "\n",
    "    def _extract_job_info_improved(self, card):\n",
    "        try:\n",
    "            # Extract basic info first\n",
    "            title = self._safe_extract(card, 'h2.jobTitle a', 'text')\n",
    "            if not title:\n",
    "                title = self._safe_extract(card, 'h2 a', 'text')\n",
    "            if title:\n",
    "                title = re.sub(r'^New\\s*', '', title).strip()\n",
    "            \n",
    "            company = self._safe_extract(card, '[data-testid=\"company-name\"]', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '.companyName', 'text')\n",
    "            if not company:\n",
    "                company = self._safe_extract(card, '[class*=\"companyName\"]', 'text')\n",
    "            \n",
    "            location = self._safe_extract(card, '[data-testid=\"text-location\"]', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '.companyLocation', 'text')\n",
    "            if not location:\n",
    "                location = self._safe_extract(card, '[class*=\"companyLocation\"]', 'text')\n",
    "            \n",
    "            # Now extract description using the basic info we have\n",
    "            description = self._extract_job_description_improved(card, title, company, location)\n",
    "            \n",
    "            # Salary - Improved extraction\n",
    "            salary = \"Salary not specified\"\n",
    "            salary_selectors = [\n",
    "                '[data-testid=\"attribute_snippet_testid\"]',\n",
    "                '.salary-snippet-container',\n",
    "                '.metadata salary-snippet-container'\n",
    "            ]\n",
    "            \n",
    "            for selector in salary_selectors:\n",
    "                salary_text = self._safe_extract(card, selector, 'text')\n",
    "                if salary_text and ('$' in salary_text or 'hour' in salary_text.lower() or 'year' in salary_text.lower()):\n",
    "                    salary = salary_text\n",
    "                    break\n",
    "            \n",
    "            # Link\n",
    "            link = self._safe_extract(card, 'h2.jobTitle a', 'href')\n",
    "            if not link:\n",
    "                link = self._safe_extract(card, 'h2 a', 'href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = 'https://ca.indeed.com' + link\n",
    "            \n",
    "            # Job Type - Improved extraction\n",
    "            job_type = self._extract_job_type_improved(card)\n",
    "            \n",
    "            if title:\n",
    "                return {\n",
    "                    'title': title,\n",
    "                    'company': company or \"Company not found\",\n",
    "                    'location': location or \"Location not specified\",\n",
    "                    'salary': salary,\n",
    "                    'job_type': job_type,\n",
    "                    'link': link or \"Link not available\",\n",
    "                    'description': description or \"Description not available\",\n",
    "                    'scraped_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job info: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _extract_job_description_improved(self, card, title, company, location):\n",
    "        \"\"\"Improved job description extraction with proper parameters\"\"\"\n",
    "        try:\n",
    "            # Try the job snippet/summary section first\n",
    "            description_selectors = [\n",
    "                'div.job-snippet',\n",
    "                'div[class*=\"job-snippet\"]',\n",
    "                'div[class*=\"snippet\"]',\n",
    "                'div[class*=\"summary\"]',\n",
    "                'ul[style*=\"list-style-type:circle\"]',\n",
    "                '.css-e9ucx3',  # From the HTML structure\n",
    "                '[data-testid=\"belowJobSnippet\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in description_selectors:\n",
    "                description = self._safe_extract(card, selector, 'text')\n",
    "                if description and len(description.strip()) > 20:\n",
    "                    description = description.strip()\n",
    "                    description = re.sub(r'\\s+', ' ', description)\n",
    "                    return description[:1000]  # Limit to 1000 characters\n",
    "            \n",
    "            # Try to extract from the entire card text with better filtering\n",
    "            card_text = card.text\n",
    "            if card_text:\n",
    "                lines = [line.strip() for line in card_text.split('\\n') if line.strip()]\n",
    "                \n",
    "                # Look for description content after basic info\n",
    "                basic_info_indicators = ['$', 'salary', 'full-time', 'part-time', 'contract', \n",
    "                                       'remote', 'temporary', 'permanent', 'urgently hiring',\n",
    "                                       'apply', 'save', 'bookmark', 'easily apply']\n",
    "                \n",
    "                description_lines = []\n",
    "                \n",
    "                for line in lines:\n",
    "                    line_lower = line.lower()\n",
    "                    \n",
    "                    # Skip if it's basic info we've already captured\n",
    "                    if any(indicator in line_lower for indicator in basic_info_indicators):\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip if it's title, company, or location (usually shorter lines that match exactly)\n",
    "                    if (title and title.lower() in line_lower) or \\\n",
    "                       (company and company.lower() in line_lower) or \\\n",
    "                       (location and location.lower() in line_lower):\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip very short lines (likely navigation or buttons)\n",
    "                    if len(line) < 20:\n",
    "                        continue\n",
    "                        \n",
    "                    # If we find a reasonably long line that's not basic info, consider it description\n",
    "                    if len(line) > 30:\n",
    "                        description_lines.append(line)\n",
    "                \n",
    "                if description_lines:\n",
    "                    description = ' '.join(description_lines[:3])  # Take first 3 description lines\n",
    "                    description = re.sub(r'\\s+', ' ', description).strip()\n",
    "                    if len(description) > 30:\n",
    "                        return description[:800]\n",
    "            \n",
    "            return \"Description snippet not available\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting description: {e}\")\n",
    "            return \"Error extracting description\"\n",
    "\n",
    "    def _extract_job_type_improved(self, card):\n",
    "        \"\"\"Improved job type extraction\"\"\"\n",
    "        try:\n",
    "            card_text = card.text.lower()\n",
    "            \n",
    "            # Check for job type in metadata\n",
    "            metadata_selectors = [\n",
    "                '[data-testid=\"attribute_snippet_testid\"]',\n",
    "                '.metadata',\n",
    "                '.css-5ooe72'\n",
    "            ]\n",
    "            \n",
    "            for selector in metadata_selectors:\n",
    "                try:\n",
    "                    elements = card.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        text = element.text.lower()\n",
    "                        if 'full-time' in text:\n",
    "                            return 'Full-time'\n",
    "                        elif 'part-time' in text:\n",
    "                            return 'Part-time'\n",
    "                        elif 'contract' in text:\n",
    "                            return 'Contract'\n",
    "                        elif 'temporary' in text:\n",
    "                            return 'Temporary'\n",
    "                        elif 'permanent' in text:\n",
    "                            return 'Permanent'\n",
    "                        elif 'remote' in text:\n",
    "                            return 'Remote'\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Fallback to text analysis\n",
    "            if 'full-time' in card_text:\n",
    "                return 'Full-time'\n",
    "            elif 'part-time' in card_text:\n",
    "                return 'Part-time'\n",
    "            elif 'contract' in card_text:\n",
    "                return 'Contract'\n",
    "            elif 'temporary' in card_text:\n",
    "                return 'Temporary'\n",
    "            elif 'remote' in card_text:\n",
    "                return 'Remote'\n",
    "            else:\n",
    "                return 'Not specified'\n",
    "                \n",
    "        except:\n",
    "            return 'Not specified'\n",
    "\n",
    "    def _safe_extract(self, parent, selector, attribute='text'):\n",
    "        try:\n",
    "            element = parent.find_element(By.CSS_SELECTOR, selector)\n",
    "            if attribute == 'text':\n",
    "                return element.text.strip()\n",
    "            else:\n",
    "                return element.get_attribute(attribute)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def save_to_database(self):\n",
    "        if not self.jobs_data or not self.db_connection:\n",
    "            print(\"No data to save or database not connected\")\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            cursor = self.db_connection.cursor()\n",
    "            saved_count = 0\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT OR IGNORE INTO jobs (title, company, location, salary, job_type, link, description, scraped_date)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            for job in self.jobs_data:\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (\n",
    "                        job['title'],\n",
    "                        job['company'],\n",
    "                        job['location'],\n",
    "                        job['salary'],\n",
    "                        job['job_type'],\n",
    "                        job['link'],\n",
    "                        job['description'],\n",
    "                        job['scraped_date']\n",
    "                    ))\n",
    "                    if cursor.rowcount > 0:\n",
    "                        saved_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting job: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            self.db_connection.commit()\n",
    "            cursor.close()\n",
    "            return saved_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to database: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def query_jobs_from_database(self, limit=10):\n",
    "        if not self.db_connection:\n",
    "            print(\"Database not connected\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Reconnect if needed\n",
    "            if not os.path.exists(\"pathfinder_db.sqlite\"):\n",
    "                return []\n",
    "                \n",
    "            conn = sqlite3.connect(\"pathfinder_db.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT title, company, location, salary, job_type, link, description, scraped_date \n",
    "                FROM jobs \n",
    "                ORDER BY scraped_date DESC \n",
    "                LIMIT ?\n",
    "            \"\"\", (limit,))\n",
    "            \n",
    "            results = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            jobs = []\n",
    "            for row in results:\n",
    "                jobs.append({\n",
    "                    'title': row[0],\n",
    "                    'company': row[1],\n",
    "                    'location': row[2],\n",
    "                    'salary': row[3],\n",
    "                    'job_type': row[4],\n",
    "                    'link': row[5],\n",
    "                    'description': row[6],\n",
    "                    'scraped_date': row[7]\n",
    "                })\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error querying database: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_database_stats(self):\n",
    "        if not os.path.exists(\"pathfinder_db.sqlite\"):\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"pathfinder_db.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Total jobs count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "            total_jobs = cursor.fetchone()[0]\n",
    "            \n",
    "            # Companies count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT company) FROM jobs\")\n",
    "            unique_companies = cursor.fetchone()[0]\n",
    "            \n",
    "            # Locations count\n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT location) FROM jobs\")\n",
    "            unique_locations = cursor.fetchone()[0]\n",
    "            \n",
    "            # Jobs with descriptions\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM jobs WHERE description != 'Description snippet not available' AND description != 'Error extracting description'\")\n",
    "            jobs_with_descriptions = cursor.fetchone()[0]\n",
    "            \n",
    "            # Latest scrape date\n",
    "            cursor.execute(\"SELECT MAX(scraped_date) FROM jobs\")\n",
    "            latest_scrape = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'total_jobs': total_jobs,\n",
    "                'unique_companies': unique_companies,\n",
    "                'unique_locations': unique_locations,\n",
    "                'jobs_with_descriptions': jobs_with_descriptions,\n",
    "                'latest_scrape': latest_scrape\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting database stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def save_to_csv(self, filename=\"indeed_jobs_backup.csv\"):\n",
    "        if not os.path.exists(\"pathfinder_db.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"pathfinder_db.sqlite\")\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM jobs\")\n",
    "            \n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(f\"Database backed up to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error backing up to CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "    def export_to_excel(self, filename=\"indeed_jobs.xlsx\"):\n",
    "        if not os.path.exists(\"pathfinder_db.sqlite\"):\n",
    "            print(\"Database file not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(\"pathfinder_db.sqlite\")\n",
    "            df = pd.read_sql_query(\"SELECT * FROM jobs\", conn)\n",
    "            df.to_excel(filename, index=False)\n",
    "            conn.close()\n",
    "            print(f\"Database exported to {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting to Excel: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    # Initialize scraper\n",
    "    scraper = ImprovedIndeedScraper(headless=True)\n",
    "    \n",
    "    # Setup SQLite database\n",
    "    if not scraper.setup_sqlite_database(\"pathfinder_db.sqlite\"):\n",
    "        print(\"Failed to setup database. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    job_title = \"\"  \n",
    "    location = \"Canada\"\n",
    "    pages = 3  # Start with fewer pages for testing\n",
    "    \n",
    "    print(f\"\\nScraping all jobs in '{location}' ({pages} pages)...\")\n",
    "    \n",
    "    # Scrape and save to database\n",
    "    start_time = time.time()\n",
    "    jobs = scraper.scrape_jobs(\n",
    "        job_title=job_title,\n",
    "        location=location,\n",
    "        max_pages=pages\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    if jobs:\n",
    "        print(f\"\\nSuccessfully processed jobs in {end_time - start_time:.1f} seconds\")\n",
    "\n",
    "        # Backup to CSV\n",
    "        scraper.save_to_csv(\"indeed_jobs_backup.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No jobs were scraped\")\n",
    "    \n",
    "    print(f\"\\nDatabase file: pathfinder_db.sqlite\")\n",
    "    print(\"CSV backup: indeed_jobs_backup.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e2045-e5f3-409e-9316-af1f058982d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
